---
title: "Foraging for Data in the Wild 2025"
author: "Daniel Perez & Jori Kandra"
---

## Unemployment Insurance Claims by State

### 1) Import libraries

```{r message=FALSE}
library(tidyverse)
library(data.table) # setnames(), map DOL data dictionary to the raw data
library(lubridate) # data helper functions to recast messy data as date type
library(openxlsx2) # mapping data and setting formatting for excel wb
```

### 2) Download raw data

UI IC & CC (NSA) comes for [ETA 539](https://oui.doleta.gov/unemploy/csv/ar539.csv), which can be found on the [DOL ETA website](https://oui.doleta.gov/unemploy/DataDownloads.asp).

Use the command-line utility function wget to trigger and direct the download of exportable online data. Wrap the statement in system() to direct execution to the terminal

(If wget doesn't work on your machine, you can download the file manually and place it in your working directory)


```{r}
# "wget -N" omits download if data has not been updated "-P" sets the file destination"
system("wget -N https://oui.doleta.gov/unemploy/csv/ar539.csv  -P data/")

```

### 3) Wrangle data

Replace raw variable names using a user-defined data dictionary. The data dictionary is a combination of the [DOL ETA 539 Data Map](https://oui.doleta.gov/dmstree/handbooks/402/402_4/4024c6/4024c6.pdf#ETA539) (found on [Data Downloads](https://oui.doleta.gov/unemploy/DataDownloads.asp) page) and the [ETA 401 handook](https://www.dol.gov/sites/dolgov/files/ETA/handbooks/2017/ETHand401_5th.pdf) "Item by Item Instructions."

```{r}
# read in data dictionary
data_dictionary <- read.csv("data/eta539_var_names.csv")
```

### 4) Cleanse and manipulate data

Use `data.table::setnames()` to apply the data dictionary to the raw data. Recast date columns to be of `Date` class type (or data storage type). "Wild data" often stores dates as `str` (or string) class type, so use functions from the `?lubridate` package to easily handle and manipulate date types.

```{r}
# Cleanse raw data
raw_data <- read.csv("data/ar539.csv") |>
  # use $ operator to select column from data frame
  setnames(old = data_dictionary$dol_code, new = data_dictionary$dol_title) |>
  # format date as class 'Date' 
  mutate(report_date = mdy(report_date),
         reflect_week_ending = mdy(reflect_week_ending))
```

### 5) Analyze

Use `dplyr::mutate()` to create new columns. Calculate non-seasonally adjusted initial claims as state UI initial claims + short-term compensation (or workshare) initial claims and non-seasonally adjusted continued claims as state UI continued claims + short-tern compensation (or workshare) continued claims.

```{r}
# Initial claims (NSA) 
initial_claims <- raw_data  |> 
  # Initial Claims & Continued Claims, non seasonally adjusted (as seen here: https://oui.doleta.gov/unemploy/claims.asp) 
  # UI IC is calculated from c3 (initial claims) & c7 (short time compensation workshare)
  mutate(nsa_initial_claims = state_ui_initial_claims + stc_workshare_equivalent_initial_claims) |> 
  select(state, report_date, nsa_initial_claims) |> 
  # filter out unstable reporting
  filter(report_date >= '1987-01-01') |>
  # transform into wide format - each state is own column
  #note: https://bookdown.org/Maxine/r4ds/pivoting.html
  pivot_wider(id_cols = report_date, names_from = state, values_from = nsa_initial_claims) |> 
  # remove Puerto Rico and US Virgin Islands
  select(-PR, -VI) |> 
  # replace state abbreviation with state name
  setnames(old = state.abb, new = state.name) |> 
  # rename DC (not included in state* utility functions)
  rename(`District of Columbia` = DC) |> 
  # sort data
  arrange(report_date)

# Continued claims (NSA) 
continued_claims <- raw_data |> 
  # Initial Claims & Continued Claims, non seasonally adjusted (as seen here: https://oui.doleta.gov/unemploy/claims.asp) 
  # UI CC is calculated from c8 & c12
  mutate(nsa_continued_claims = state_ui_adjusted_continued_weeks_claimed + stc_workshare_equivalent_continued_weeks_claimed) |> 
  select(state, reflect_week_ending, nsa_continued_claims) |> 
  # filter out unstable reporting
  filter(reflect_week_ending >= '1987-01-01') |>
  # transform into wide format - each state is own column
  pivot_wider(id_cols = reflect_week_ending, names_from = state, values_from = nsa_continued_claims) |> 
  # remove Puerto Rico & US Virgin Islands
  select(-PR, -VI) |> 
  # replace state abbreviation with state name
  setnames(old = state.abb, new = state.name) |> 
  # replace DC (not included in state.* utility data)
  rename(`District of Columbia` = DC) |> 
  # sort data 
  arrange(reflect_week_ending)
```

### 6) Export data

Use `?openxlsx2` to create excel workbooks, which support multiple tabs and backend formatting. This is a great way to generate replicable final products. Note that openxlsx2 uses the `$` pipe operator to modify workbook objects created by `openxlsx2::wb_workbook()`. Create worksheets, add data, and use functions such as `opnexlsx2::wb_set_col_widths()` and `openxlsx2::add_cell_style()` to stylize the workbook.

```{r}
# create WB object
wb <- wb_workbook()

# write UI state IC to WB object
#note: $ - pipe operator in openxlsx2 
wb$
  # add new worksheet
  add_worksheet(sheet = "Initial claims")$
  # add data to worksheet
  add_data(x = initial_claims)$
  # set columm widths
  set_col_widths(cols = 2:ncol(initial_claims), widths = 15)$
  # format column headers
  add_cell_style(dims = wb_dims(rows = 1, cols = 2:ncol(initial_claims)), 
                 wrap_text = TRUE, horizontal = "center", vertical = "center")$
  # repeat for continued claims
  add_worksheet(sheet = "Continued claims")$
  add_data(x = continued_claims)$
  set_col_widths(cols = 2:ncol(continued_claims), widths = 15)$
  add_cell_style(dims = wb_dims(rows = 1, cols = 2:ncol(continued_claims)), 
                 wrap_text = TRUE, horizontal = "center", vertical = "center")$
  # save workbook to output folder
  save("output/state_ui.xlsx")
```

## Using the QCEW to measure employment growth in data centers by state

> **Objectives**
>
> 1.  Create data by iteratively calling a function.
> 2.  Bind/append data frames to create a large dataset.
> 3.  Read data from a .CSV directly from the web into R.
> 4.  Harmonize data types.
> 5.  Use joins to combine datasets.
> 6.  Filter using string detection.
> 7.  Reorder variables using `select()`, `arrange()`, and/or `relocate()` functions.
> 8.  Use some tricks to create quarterly and monthly data types with `lubridate`.
> 9.  Measure employment changes for NAICS industry **518**.
> 10. Bonus: quick visualization with **ggplot2**!

**Question.** How has “data center” employment (NAICS 518) grown since late 2022, and in which states has it grown the most?

To answer these questions, we’ll fetch 2022–2024 quarterly data for NAICS **518: Computing infrastructure providers, data processing, web hosting, and related services**.

\*\* Disclaimer\*\*: I'm not sure if this is the most appropriate NAICS code, but it makes for a good exercise! Depending on your research question, you may want to refine how you select industry codes (e.g., include selected sub‑industries or complementary sectors.).

## 1) Load libraries

```{r Libraries, message=FALSE}

library(tidyverse)
library(lubridate)

```

## 2) BLS functions for loading QCEW data

The BLS conveniently provides a script and three functions for R users to load QCEW data directly into R! Below are two helpers adapted for this module. They construct an API URL and return a data frame for the requested year/quarter/industry or area.

These resources can be downloaded from this page: <https://www.bls.gov/cew/additional-resources/open-data/sample-code.htm#RSCRIPT>

```{r QCEW functions}

# This function loads all industries for one geographical area
qcewGetAreaData <- function(year, qtr, area) {
  url <- "http://data.bls.gov/cew/data/api/YEAR/QTR/area/AREA.csv"
  url <- sub("YEAR", year, url, ignore.case=FALSE)
  url <- sub("QTR", tolower(qtr), url, ignore.case=FALSE)
  url <- sub("AREA", toupper(area), url, ignore.case=FALSE)
  read.csv(url, header = TRUE, sep = ",", quote="\"", dec=".", na.strings=" ", skip=0)
}

# This function loads one industry for all geographical areas
qcewGetIndustryData <- function (year, qtr, industry) {
	url <- "http://data.bls.gov/cew/data/api/YEAR/QTR/industry/INDUSTRY.csv"
	url <- sub("YEAR", year, url, ignore.case=FALSE)
	url <- sub("QTR", tolower(qtr), url, ignore.case=FALSE)
	url <- sub("INDUSTRY", industry, url, ignore.case=FALSE)
	read.csv(url, header = TRUE, sep = ",", quote="\"", dec=".", na.strings=" ", skip=0)
}

# Quick examples (not evaluated by default)
# In ex. 1, we call the qcewGetAreaData() function, passing parameters for year, quarter, and areafips/geography i.e. year = 2015, quarter = 1, areafips = 26000 or Michigan.
# We then assign the data called to a variable called MichiganData!

#  MichiganData <- qcewGetAreaData("2015", "1", "26000")
#  Construction <- qcewGetIndustryData("2015", "1", "1012")

```

## 3) QCEW data pull

Since the example functions load only one quarter at a time, we'll want to make some modifications. Instead of calling the function 12 times by hand, we’ll build a grid of parameters and map across it.

```{r Load QCEW}
# Set our parameters 
years <- 2022:2024
quarters <- 1:4
industries <- c('518')   

# create 12 combinations (3 years × 4 quarters × 1 industry) to pass through pmap()
combos <- tidyr::crossing(year = years, qtr = quarters, industry = industries)

# returns 12 dataframes, one for each combination
qcew_raw <- purrr::pmap(
  combos,
  function(year, qtr, industry) {
    df <- qcewGetIndustryData(year, qtr, industry)
    df$year <- year
    df$qtr  <- qtr
    df$industry <- industry
    df
}) |> 
# Combines all dataframes by appending or binding "rows" 
  bind_rows()

#Explore our data
glimpse(qcew_raw)

```

## 4) Add readable labels (industry & area titles)

Our dataframe is loaded! But it's not very legible. For starters, some geographic and industry titles would help.

The BLS provides a codebook for parsing our data <https://www.bls.gov/cew/about-data/downloadable-file-layouts/quarterly/naics-based-quarterly-layout.htm>. We'll load these directly into R.

\*\* disclaimer \*\* depending on your IT's security settings, you may not be able to directly download these links into R. If you encounter this issue (like I did), you can navigate directly to the .htm links below, download the .CSV files, and place them in your working directory.

Industry titles downloaded from <https://www.bls.gov/cew/classifications/industry/industry-titles.htm>

Area titles downloaded from <https://www.bls.gov/cew/classifications/areas/qcew-area-titles.htm>

```{r Load labels, message=FALSE}

# link to csv files on the BLS QCEW site
ind_title_url <- 'https://www.bls.gov/cew/classifications/industry/industry-titles.csv'
area_title_url <- 'https://www.bls.gov/cew/classifications/areas/area-titles-csv.csv'

# # Read csv files directly into R from the QCEW page
# ind_titles  <- read_csv(ind_title_url)
# area_titles <- read_csv(area_title_url)

ind_titles <- read_csv('data/industry_titles.csv')
area_titles <- read_csv('data/area-titles-csv.csv')

```

### 4a) First attempt at joining labels

We’ll first attempt a natural `left_join()` on industry_code. A left join keeps all rows from our main dataset (qcew, the "x" table) and adds matches from ind_titles (the "y" table). By default, it matches on any identically named columns (a "natural join"), but we could also set the key explicitly using the `by =` argument.

See Section [19.4 How do joins work?](https://r4ds.hadley.nz/joins.html#how-do-joins-work) from [**R for Data Science (2e)**](https://r4ds.hadley.nz/) for some great visualizations.

```{r Comparing label dfs, error=TRUE}
# Can you spot the difference?
glimpse(qcew_raw$industry_code)   # likely <int> / <dbl>
glimpse(ind_titles$industry_code) # likely <chr>

# What happens?
qcew_raw |>
  dplyr::left_join(ind_titles)
```

### 4b) Harmonizing data types to join

Maldito! We have an issue. Despite matching variable names, industry_code in datasets "x" and "y" are different datatypes. in "x", it's an integer, a numeric type. in "y" it's a character. In order to merge we need our datatypes to be the same.

Fortunately, is an easy fix. We can convert one of two columns to match data types. Which should we convert? Well, it's easier to go from character to numeric here. A quirk of R is that numerics don't have leading zeros, but strings can. You wouldn't write 100 as 0100, or 00100, right? Right??

Since NAICS 3‑digit codes like 518 don’t have leading zeros, converting ind_titles\$industry_code to numeric is safe here. (General tip: if codes are required to have leading zeros, keep them as character in both tables.)

```{r Data cleaning}

# Make the join keys the same type
ind_titles <- ind_titles |>
  mutate(industry_code = as.numeric(industry_code))

# Now the join works:
qcew_ind_clean <- qcew_raw |>
  # natural join on identical names
  left_join(ind_titles)  

# We could also be explicit about which variable to join by
# qcew_ind_clean <- qcew_raw |> 
#   left_join(ind_titles, by = join_by(industry_code))

# If our variable names differ, we could map them using 
# left_join(ind_titles, by = c("industry_code" = "ind_code"))

```

### 4c) Cleaning continued

Now join titles and keep only variables we need. We also want limit to **private‑sector, statewide** data only. We know from the QCEW codebook that: `own_code == 5` and `agglvl_code == 55`.

See this codebook for more detail: [QCEW Field Layouts for NAICS-Based, Quarterly CSV Files](https://www.bls.gov/cew/about-data/downloadable-file-layouts/quarterly/naics-based-quarterly-layout.htm)

```{r Cleaning continued}

# (If needed) ensure area_fips types match before joining area_titles
# area_titles <- area_titles |> mutate(area_fips = as.character(area_fips))

qcew_clean <- qcew_ind_clean |>
  left_join(area_titles) |> 
  # Too many variables we don't need, let's restrict and re-order using select
  select(
    year, qtr, area_fips, area_title, industry_code,
    industry_title, own_code, agglvl_code, month1_emplvl, month2_emplvl,
    month3_emplvl, total_qtrly_wages) |> 

  # Now, why are seeing two rows for each state? Again, per the codebook, QCEW contains data for public and private sector industries
  # We can filter for private sector data using own_code == 5
  filter(own_code == 5) |> 
  filter(agglvl_code == 55)

# Tip: An alternative way to filter for statewide data. If you don’t have a variable like agglvl_code, you can use string detection to filter statewide rows. We prefer to use code‑book filters when available.
# filter(str_detect(area_title, " -- Statewide")) 

```

## 5) Quarterly dates & averages

Create a quarterly average employment measure and a proper quarterly date using **lubridate**’s `yq()`.

```{r}
qcew_qtr <- qcew_clean |>
  mutate(
    qtr_avg_emp = (month1_emplvl + month2_emplvl + month3_emplvl) / 3,
    qdate       = yq(paste(year, qtr, sep = " Q"))
  )
```

## 6) Reshape to create state‑by‑column tables

Now that we have quarterly data, we'll use pivot_wider() to create a table of quarterly data that is long by date.

```{r}
state_qtr_table <- qcew_qtr |>
  mutate(state = str_replace(area_title, " -- Statewide", "")) |>
  select(qdate, state, qtr_avg_emp) |>
  pivot_wider(id_cols = qdate, names_from = state, values_from = qtr_avg_emp)

state_qtr_table
```

## 7) Monthly data from QCEW

The QCEW provides monthly employment levels in each quarter! With a few tweaks to our data frame, we can produce a monthly series, giving us a more granular look at how data center employment has grown since 2022. Each QCEW quarter reports employment for its three months. We can unpivot those columns to build a monthly time series.

```{r Create monthyl data frame}

qcew_monthly <- qcew_clean |>
  pivot_longer(
    cols = starts_with("month"),
    names_to = "month_in_qtr",
    names_pattern = "month(\\d+)_emplvl",
    values_to = "emplvl"
  ) |>
  mutate(
    month_in_qtr = as.integer(month_in_qtr),
    month        = (qtr - 1) * 3 + month_in_qtr,
    date         = make_date(year, month, 1),
  ) |>
  select(area_title, area_fips, industry_code, year, qtr, date, emplvl) |>
  arrange(area_title, year, qtr, date) |> 
  # let's clean up our state names!
  mutate(state = str_replace(area_title, " -- Statewide", ""))

head(qcew_monthly)
```

## 8) Measure employment growth since late 2022

For a simple comparison, compute percentage change from **November 2022** to the latest available month for each state. (If a state is missing November specifically, we’ll use the first available month on or after 2022‑11‑01.)

```{r Growth}

state_growth <- qcew_monthly |>
  summarize(
    start_emplvl = first(emplvl),
    end_emplvl   = last(emplvl),
    start_date   = first(date),
    end_date     = last(date),
    .by=state) |>
  mutate(
    emp_change = (end_emplvl - start_emplvl),
    pct_change = (end_emplvl / start_emplvl - 1) * 100) |>
  # arrange data in descending order by pct_change
  arrange(desc(pct_change))

state_growth |> slice_head(n = 10)
```

## 9) Bonus: Making a quick visualization

Lets plot a few states using ggplot2

P.S. check out [viz_workshop.qmd](viz_workshop.qmd) for some more ggplot examples!

```{r ggplots}

sel_state_data <- qcew_monthly |> 
  filter(state %in% c('Texas', 'Arkansas', 'Louisiana'))

ggplot(data = sel_state_data, aes(x=date, y=emplvl, color=state)) +
  geom_line() +
  labs(
    title = "Monthly employment (NAICS 518)",
    x = NULL, y = "Employment level",
    color = "State"
  ) 

```

## 10) Notes on reproducibility

> Pro-tips: - This module reads files directly from **bls.gov**; those URLs occasionally change. If a link breaks, visit the QCEW classifications pages to refresh the URLs. - Consult **QCEW layout/codebook** to confirm variable meanings & aggregation levels for your projects.

Happy coding!🕺
