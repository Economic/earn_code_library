[
  {
    "objectID": "weighted_percentiles.html",
    "href": "weighted_percentiles.html",
    "title": "Weighted percentiles",
    "section": "",
    "text": "This script uses Current Population Survey (CPS) microdata extracts to calculate sample weighted wage percentiles over time.",
    "crumbs": [
      "R code",
      "Weighted percentiles"
    ]
  },
  {
    "objectID": "weighted_percentiles.html#preliminaries",
    "href": "weighted_percentiles.html#preliminaries",
    "title": "Weighted percentiles",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst, load the required packages:\n\nlibrary(tidyverse)\nlibrary(MetricsWeighted)\nlibrary(epiextractr)\n\nThen grab wage earner observations from the 1979-2022 CPS ORG data using epiextractr. If necessary, use the .extracts_dir argument of load_org() to point it to your downloaded CPS extracts.\n\ncps_data &lt;- epiextractr::load_org(1979:2022, year, orgwgt, wage) %&gt;% \n  filter(wage &gt; 0)",
    "crumbs": [
      "R code",
      "Weighted percentiles"
    ]
  },
  {
    "objectID": "weighted_percentiles.html#goal-and-quick-solution",
    "href": "weighted_percentiles.html#goal-and-quick-solution",
    "title": "Weighted percentiles",
    "section": "Goal and quick solution",
    "text": "Goal and quick solution\nLet’s calculate the 10th, 50th, and 90th wage percentiles for each year, where these will be sample weighted percentiles using the orgwgt variable as the weight.\nFirst I’ll show you how you might do that and then I’ll break it down step-by-step.\n\n# percentiles of interest\np &lt;- c(10, 50, 90)\n\n# calculate percentiles\ncps_data %&gt;% \n  reframe(\n    percentile = p, \n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  )\n\n# A tibble: 132 × 3\n    year percentile value\n   &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  1979         10  2.90\n 2  1979         50  5   \n 3  1979         90 10.1 \n 4  1980         10  3.10\n 5  1980         50  5.62\n 6  1980         90 11.2 \n 7  1981         10  3.35\n 8  1981         50  6.05\n 9  1981         90 12.5 \n10  1982         10  3.35\n# ℹ 122 more rows",
    "crumbs": [
      "R code",
      "Weighted percentiles"
    ]
  },
  {
    "objectID": "weighted_percentiles.html#step-by-step-explanation",
    "href": "weighted_percentiles.html#step-by-step-explanation",
    "title": "Weighted percentiles",
    "section": "Step-by-step explanation",
    "text": "Step-by-step explanation\nA simple version of this problem would be to calculate the median wage in 2022.\n\ncps_data %&gt;% \n  filter(year == 2022) %&gt;% \n  summarize(p_50 = median(wage))\n\n# A tibble: 1 × 1\n   p_50\n  &lt;dbl&gt;\n1    23\n\n\nUse weighted_median() from the MetricsWeighted package to calculate a sample-weighted median.\n\ncps_data %&gt;% \n  filter(year == 2022) %&gt;% \n  summarize(p_50 = weighted_median(wage, w = orgwgt))\n\n# A tibble: 1 × 1\n   p_50\n  &lt;dbl&gt;\n1  22.9\n\n\nUse weighted_quantile() and the probs argument to calculate any weighted percentile. Note that probs ranges from 0 to 1.\n\ncps_data %&gt;% \n  filter(year == 2022) %&gt;% \n  summarize(p_10 = weighted_quantile(wage, w = orgwgt, probs = 0.10))\n\n# A tibble: 1 × 1\n   p_10\n  &lt;dbl&gt;\n1  12.5\n\n\nTo calculate multiple percentiles provide, provide a vector of percentiles and also switch from summarize() to reframe() to allow multiple rows of results, as opposed to a single summary row.\n\np &lt;- c(10, 50, 90)\n\ncps_data %&gt;% \n  filter(year == 2022) %&gt;% \n  reframe(\n    percentile = p, \n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100)\n  )\n\n# A tibble: 3 × 2\n  percentile value\n       &lt;dbl&gt; &lt;dbl&gt;\n1         10  12.5\n2         50  22.9\n3         90  57.7\n\n\nNotice how we used probs = p / 100 in the arguments to weighted_quantile().\nFinally, to calculate percentiles for each year, we can use the .by argument of reframe.\n\ncps_data %&gt;% \n  reframe(\n    percentile = p, \n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  ) \n\n# A tibble: 132 × 3\n    year percentile value\n   &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  1979         10  2.90\n 2  1979         50  5   \n 3  1979         90 10.1 \n 4  1980         10  3.10\n 5  1980         50  5.62\n 6  1980         90 11.2 \n 7  1981         10  3.35\n 8  1981         50  6.05\n 9  1981         90 12.5 \n10  1982         10  3.35\n# ℹ 122 more rows\n\n\nObserve the shape of the resulting output dataset: it is long in both years and percentiles. Long data like this is useful for more data manipulation or for making plots.\nFor example, suppose you wanted to plot nominal wage growth since 2000.\n\n# construct the percentiles in long format\npercentile_data &lt;- cps_data %&gt;% \n  reframe(\n    percentile = p, \n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  )\n\n# grab the 2000 base values\nbase_values &lt;- percentile_data %&gt;% \n  filter(year == 2000) %&gt;% \n  select(percentile, base_value = value)\n\npercentile_data %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  full_join(base_values, by = \"percentile\") %&gt;% \n  mutate(\n    wage_growth = value / base_value - 1,\n    percentile = paste0(percentile, \"th percentile\")\n  ) %&gt;% \n  ggplot(aes(x = year, y = wage_growth, color = percentile)) + \n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhile long data like that is useful for additional analysis, if you need to see more of the data at once, like for a table, you might want to make the data wider. With pivot_wider() you can reshape the data so that it is long in years and wide in percentiles.\n\ncps_data %&gt;%\n  reframe(\n    percentile = p,\n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  ) %&gt;%\n  pivot_wider(id_cols = year, names_from = percentile, values_from = value)\n\n# A tibble: 44 × 4\n    year  `10`  `50`  `90`\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1979  2.90  5     10.1\n 2  1980  3.10  5.62  11.2\n 3  1981  3.35  6.05  12.5\n 4  1982  3.35  6.5   13.1\n 5  1983  3.40  6.70  14  \n 6  1984  3.5   7     15.0\n 7  1985  3.5   7.47  15  \n 8  1986  3.60  7.5   16  \n 9  1987  3.75  8     16.8\n10  1988  4     8.02  17.5\n# ℹ 34 more rows\n\n\nOf course, the column names are pretty ugly. You could add a “th” to the column names from the get-go.\n\ncps_data %&gt;%\n  reframe(\n    percentile = paste0(p, \"th\"),\n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  ) %&gt;%\n  pivot_wider(id_cols = year, names_from = percentile, values_from = value)\n\n# A tibble: 44 × 4\n    year `10th` `50th` `90th`\n   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  1979   2.90   5      10.1\n 2  1980   3.10   5.62   11.2\n 3  1981   3.35   6.05   12.5\n 4  1982   3.35   6.5    13.1\n 5  1983   3.40   6.70   14  \n 6  1984   3.5    7      15.0\n 7  1985   3.5    7.47   15  \n 8  1986   3.60   7.5    16  \n 9  1987   3.75   8      16.8\n10  1988   4      8.02   17.5\n# ℹ 34 more rows\n\n\nOr you could make the column names more data analysis friendly with a “p_” prefix.\n\ncps_data %&gt;%\n  reframe(\n    percentile = p,\n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  ) %&gt;%\n  pivot_wider(\n    id_cols = year,\n    names_from = percentile,\n    values_from = value,\n    names_prefix = \"p_\"\n  )\n\n# A tibble: 44 × 4\n    year  p_10  p_50  p_90\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1979  2.90  5     10.1\n 2  1980  3.10  5.62  11.2\n 3  1981  3.35  6.05  12.5\n 4  1982  3.35  6.5   13.1\n 5  1983  3.40  6.70  14  \n 6  1984  3.5   7     15.0\n 7  1985  3.5   7.47  15  \n 8  1986  3.60  7.5   16  \n 9  1987  3.75  8     16.8\n10  1988  4     8.02  17.5\n# ℹ 34 more rows",
    "crumbs": [
      "R code",
      "Weighted percentiles"
    ]
  },
  {
    "objectID": "weighted_percentiles.html#extra-credit",
    "href": "weighted_percentiles.html#extra-credit",
    "title": "Weighted percentiles",
    "section": "Extra credit",
    "text": "Extra credit\nConsider the concise code\n\ncps_data %&gt;%\n  reframe(\n    name = p,\n    value = weighted_quantile(wage, w = orgwgt, probs = p / 100),\n    .by = year\n  ) %&gt;%\n  pivot_wider(id_cols = year, names_prefix = \"p_\")\n\n# A tibble: 44 × 4\n    year  p_10  p_50  p_90\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1979  2.90  5     10.1\n 2  1980  3.10  5.62  11.2\n 3  1981  3.35  6.05  12.5\n 4  1982  3.35  6.5   13.1\n 5  1983  3.40  6.70  14  \n 6  1984  3.5   7     15.0\n 7  1985  3.5   7.47  15  \n 8  1986  3.60  7.5   16  \n 9  1987  3.75  8     16.8\n10  1988  4     8.02  17.5\n# ℹ 34 more rows\n\n\nWhy does it produce the same results as the longer code above?",
    "crumbs": [
      "R code",
      "Weighted percentiles"
    ]
  },
  {
    "objectID": "epi_extracts_webinar.html",
    "href": "epi_extracts_webinar.html",
    "title": "Using EPI’s CPS Extracts in R",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on September 16th, 2025.\nMissed this talk? See the recording here: Using EPI’s CPS Microdata Extracts in R. Passcode: 3xtracts2025!\nAdditional links:\n\nPresentation slides: https://economic.github.io/zipperer_2025_webinar_epiextracts\nEPI CPS extracts documentation https://microdata.epi.org/\nepiextractr package https://economic.github.io/epiextractr/\nEARN Code Library Load EPI CPS Extracts via epiextractr page",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Using EPI's CPS Extracts in R"
    ]
  },
  {
    "objectID": "epi_extracts_webinar.html#welcome-to-the-using-the-epi-cps-microdata-extracts-in-r-landing-page",
    "href": "epi_extracts_webinar.html#welcome-to-the-using-the-epi-cps-microdata-extracts-in-r-landing-page",
    "title": "Using EPI’s CPS Extracts in R",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on September 16th, 2025.\nMissed this talk? See the recording here: Using EPI’s CPS Microdata Extracts in R. Passcode: 3xtracts2025!\nAdditional links:\n\nPresentation slides: https://economic.github.io/zipperer_2025_webinar_epiextracts\nEPI CPS extracts documentation https://microdata.epi.org/\nepiextractr package https://economic.github.io/epiextractr/\nEARN Code Library Load EPI CPS Extracts via epiextractr page",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Using EPI's CPS Extracts in R"
    ]
  },
  {
    "objectID": "epi_extracts_webinar.html#final-cps-script",
    "href": "epi_extracts_webinar.html#final-cps-script",
    "title": "Using EPI’s CPS Extracts in R",
    "section": "Final CPS script",
    "text": "Final CPS script\n\nlibrary(tidyverse)\nlibrary(epiextractr)\n\n\n# low wage threshold\nlow_wage_threshold = 20\n\n# use the CPS ORG\norg_data = load_org(2024, year, orgwgt, wage, wageotc, statefips, female, wbhao)\n\nUsing EPI CPS ORG Extracts, Version 2025.9.11\n\nga_data = org_data |&gt; \n  # wage earners only\n  filter(wageotc &gt; 0) |&gt; \n  # in Georgia\n  filter(statefips == 13) |&gt; \n  # 2024 only\n  filter(year == 2024) |&gt; \n  # low wage indicator\n  mutate(low_wage = if_else(wageotc &lt; low_wage_threshold, 1, 0))\n\n# number low-wage in Georgia\nga_data |&gt; \n  count(low_wage, wt = orgwgt / 12)\n\n# A tibble: 2 × 2\n  low_wage        n\n     &lt;dbl&gt;    &lt;dbl&gt;\n1        0 2904705.\n2        1 1668287.\n\n# share low-wage\nga_data |&gt; \n  summarize(\n    weighted.mean(low_wage, w = orgwgt),\n    .by = wbhao\n  )\n\n# A tibble: 5 × 2\n  wbhao        `weighted.mean(low_wage, w = orgwgt)`\n  &lt;int+lbl&gt;                                    &lt;dbl&gt;\n1 1 [White]                                    0.305\n2 2 [Black]                                    0.384\n3 3 [Hispanic]                                 0.557\n4 4 [Asian]                                    0.299\n5 5 [Other]                                    0.494\n\n# analysis by gender\nga_data |&gt; \n  count(female, wt = orgwgt / 12)\n\n# A tibble: 2 × 2\n  female            n\n  &lt;int+lbl&gt;     &lt;dbl&gt;\n1 0 [Male]   2309205.\n2 1 [Female] 2263787.\n\n# analysis by race\nga_data |&gt; \n  count(wbhao, wt = orgwgt / 12)\n\n# A tibble: 5 × 2\n  wbhao               n\n  &lt;int+lbl&gt;       &lt;dbl&gt;\n1 1 [White]    2197534.\n2 2 [Black]    1486286.\n3 3 [Hispanic]  612498.\n4 4 [Asian]     258840.\n5 5 [Other]      17833.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Using EPI's CPS Extracts in R"
    ]
  },
  {
    "objectID": "bootcamp_preview.html",
    "href": "bootcamp_preview.html",
    "title": "EARNCon 2023 Bootcamp Preview",
    "section": "",
    "text": "Welcome to the EARNCon 2023 data bootcamp preview page!\nThis step-by-step guide will have you fully prepared to participate in the R data bootcamp and data workshops on benchmarking and data visualization."
  },
  {
    "objectID": "bootcamp_preview.html#agenda",
    "href": "bootcamp_preview.html#agenda",
    "title": "EARNCon 2023 Bootcamp Preview",
    "section": "Agenda",
    "text": "Agenda\nFacilitators: Daniel Perez & Sebastian Hickey\nEARNTalk date: Thursday, September 21st, 2023. 2-3:00 p.m. ET\nProcess:\n\nEARN Code Library preview\nDownloading and installing R and RStudio Desktop\nExploring the RStudio user interface\n\nDemonstrate\n\nSetting a working directory\nCreating a project\nDownloading packages that will be used at EARNCon\nCreating a script\n\n\nRegistering for Tableau Public\n\n\nEARN Code Library preview\nWe’re glad you’re here. the EARN Code Library is still in the early stages of development. As such, we wholeheartedly welcome your input on what content would be most useful.\n\n\nDownloading and installing R and Rstudio desktop\nInstalling R and RStudio on work machines can sometimes be challenging due to a variety of reasons.\n\nSome common issues (and resolutions):\n\nAdministrative Privileges: Many work machines have restricted administrative rights. This can prevent users from installing software, including R and RStudio.\n\nResolution: Request temporary administrative rights from your IT department or ask them to install R and RStudio for you.\n\nFirewall and Network Restrictions: Some corporate networks have strict firewall settings or network restrictions that can block the download of R, RStudio, or packages from CRAN (Comprehensive R Archive Network).\n\nResolution: Contact your IT department to whitelist the necessary domains (like CRAN) or provide a secure network path to download and install the required software.\n\nDependency Issues: R packages often depend on other packages or system libraries. If these are not installed or are incompatible versions, it can lead to errors.\n\nResolution: Ensure that all required dependencies are installed. You can use the install.packages() function with the dependencies=TRUE argument in R to automatically install package dependencies.\n\nOperating System Specific Issues: Depending on whether you’re using Windows, macOS, or Linux, there might be OS-specific issues to consider.\n\nResolution: Refer to the official R and RStudio installation guides for your specific operating system. For Linux, tools like apt-get or yum can be used to install the necessary libraries.\n\nPackage Binary vs. Source Installation: On some systems, users might encounter issues when trying to install package binaries.\n\nResolution: Ensure you have the necessary development tools installed (like gcc on Linux or Xcode on macOS). When installing a package in R, you can use the type=\"source\" argument with install.packages() to install from source.\n\nMismatched Versions: Ensure that the version of RStudio you’re installing is compatible with the version of R you have installed.\n\nResolution: Always check the compatibility of RStudio with your R version before installation. If there’s a mismatch, consider upgrading R or downloading a compatible version of RStudio.\n\nRtools Installation (Windows only): Rtools is essential for building and installing some R packages from source on Windows.\n\nResolution: Download and install Rtools from the CRAN website. Ensure that the Rtools/bin directory is added to your system PATH.\n\n\n\n\nEnough talk, time to download!\n\nNavigate to https://posit.co/downloads/"
  },
  {
    "objectID": "median_wages_demos.html",
    "href": "median_wages_demos.html",
    "title": "State-level median wages by demographic",
    "section": "",
    "text": "Use this code to find median wages and sample sizes by various demographics (race, gender, race and gender, and educational attainment) using the EPI CPS microdata extracts.\nPlease note: these data are already available on the State of Working American Data Library. However, due to small sample sizes, some cuts of these data are suppressed. You can use this code to generate all data, regardless of sample size, but be careful when making claims about data with small sample sizes.\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code. We’ll also set up a wb value that will allow us to generate an Excel workbook.\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(epiextractr)\nlibrary(epidatatools)\nlibrary(labelled)\nlibrary(realtalk)\nlibrary(openxlsx2)\n\n#Set up workbook\nwb &lt;- wb_workbook()",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#set-up-for-inflation-adjusting",
    "href": "median_wages_demos.html#set-up-for-inflation-adjusting",
    "title": "State-level median wages by demographic",
    "section": "Set up for inflation adjusting",
    "text": "Set up for inflation adjusting\nEPI uses the CPI-U-RS series to inflation adjust wages, so we’ll select that series and assign it to a dataframe.\n\n# pulling in CPI-U-RS series\ncpi_data &lt;- realtalk::cpi_u_rs_annual\n\n# set base year to 2024\ncpi2024 &lt;- cpi_data$cpi_u_rs[cpi_data$year==2024]",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#import-and-clean-data",
    "href": "median_wages_demos.html#import-and-clean-data",
    "title": "State-level median wages by demographic",
    "section": "Import and clean data",
    "text": "Import and clean data\nNote: Don’t forget to update years to match your setup before running the script.\nRunning this script chunk will call the BLS Current Population Survey ORG data required to calculate median wages.\nWe’ll set standard labor force restrictions, ensuring that we only keep workers who are not self-employed or self-incorporated and are over the age of 16. We’ll also restrict this data to one state (Wisconsin, in this case), and inflation adjust wages.\n\n#load in CPS ORG data\ncps_org &lt;- load_cps(\"org\", 1979:2024, year, orgwgt, wage, statefips, female, wbho, age, \n                                      selfemp, emp, selfinc, cow1, educ, gradeatn) %&gt;%\n  # Standard labor force and age restrictions, restrict to only Wisconsin (statefips code = 55)\n  filter(age &gt;= 16, emp == 1, statefips == 55,\n         case_when(year &lt; 1989 ~ selfemp == 0,\n                   year &gt;= 1989 & year &lt; 1994 ~ selfemp == 0 | selfinc == 0,\n                   TRUE ~ cow1 &lt;= 5)) %&gt;% \n  # Merge annual CPI data to data frame by year\n  left_join(cpi_data, by='year') %&gt;%\n  # Inflation adjust wages to 2024$\n  mutate(realwage = wage * (cpi2024/cpi_u_rs))",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#sheet-1-wages-by-race-gender-and-race-and-gender.",
    "href": "median_wages_demos.html#sheet-1-wages-by-race-gender-and-race-and-gender.",
    "title": "State-level median wages by demographic",
    "section": "Sheet 1: Wages by race, gender, and race and gender.",
    "text": "Sheet 1: Wages by race, gender, and race and gender.\nThis section of the script produces median wages by year and by various demographic cuts. Here we use EPI methodology to correct for wage clumping by created a weighted average of wages around the median. The result is one median wage per year per demographic cut.\n\nMedian wages by gender\n\nwages_gender &lt;- cps_org |&gt; \n  mutate(female = to_factor(female)) |&gt;\n  summarise(\n      wage_median = averaged_median(\n        x = realwage, \n        w = orgwgt/12,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(female, year)) |&gt;\n  #Relabel for clarity\n  mutate(gender = case_when(female == \"Female\" ~ \"Women\", female == \"Male\" ~ \"Men\")) |&gt;\n  #Change orientation for Excel layout\n  pivot_wider(id_cols = year, names_from = gender, values_from = wage_median)\n\n\n\nMedian wages by race\n\nwages_race &lt;- cps_org |&gt; \n  mutate(wbho = to_factor(wbho)) |&gt;\n  summarise(\n      wage_median = averaged_median(\n        x = realwage, \n        w = orgwgt/12,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(wbho, year)) |&gt;\n  #Change orientation for Excel layout\n  pivot_wider(id_cols = year, names_from = wbho, values_from = wage_median)\n\n\n\nMedian wages by race and gender\n\nwages_race_gender &lt;- cps_org |&gt; \n  mutate(wbho = to_factor(wbho)) |&gt;\n  mutate(female = to_factor(female)) |&gt;\n  summarise(\n      wage_median = averaged_median(\n        x = realwage, \n        w = orgwgt/12,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(wbho, female, year)) |&gt;\n  unite(col = \"demographic\", c(wbho, female), na.rm = TRUE) |&gt; \n  #Relabel for clarity\n  mutate(demographic = case_when(demographic == \"White_Female\" ~ \"White Women\", \n                                demographic == \"White_Male\" ~ \"White Men\",\n                                demographic == \"Black_Female\" ~ \"Black Women\", \n                                demographic == \"Black_Male\" ~ \"Black Men\",\n                                demographic == \"Hispanic_Female\" ~ \"Hispanic Women\", \n                                demographic == \"Hispanic_Male\" ~ \"Hispanic Men\",\n                                demographic == \"Other_Female\" ~ \"Other Women\", \n                                demographic == \"Other_Male\" ~ \"Other Men\")) |&gt;\n  #Change orientation for Excel layout\n  pivot_wider(id_cols = year, names_from = demographic, values_from = wage_median)\n\n\n\nFormat and save output\nHere we’ll join all three dataframes into one, order our variables, and save the new dataframe to a sheet in our Excel workbook.\n\n#Join the three dataframes by year\nwage_demos &lt;-  left_join(wages_gender, wages_race, by='year') |&gt;\n  left_join(wages_race_gender) |&gt; \n  #Order the columns\n  select(year, starts_with(\"White\"), starts_with(\"Black\"), starts_with(\"Hispanic\"), Women, Men)\n\n#Create a new sheet and add data to sheet\nwb$add_worksheet(sheet = \"Race and gender_median wages\") $\n  add_data(x = wage_demos)",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#sheet-2-sample-sizes",
    "href": "median_wages_demos.html#sheet-2-sample-sizes",
    "title": "State-level median wages by demographic",
    "section": "Sheet 2: Sample sizes",
    "text": "Sheet 2: Sample sizes\nThis code chunk follows the exact same steps as the section above, except it generates sample sizes instead of median wages. It is extremely important to reference this sheet when making claims about the data. Avoid relying on data drawn from small sample sizes.\n\n#By gender\nsample_gender &lt;- cps_org |&gt; \n  mutate(female = to_factor(female)) |&gt;\n  summarise(n=n(),\n        .by=c(female, year)) |&gt;\n  mutate(gender = case_when(female == \"Female\" ~ \"Women\", female == \"Male\" ~ \"Men\")) |&gt;\n  pivot_wider(id_cols = year, names_from = gender, values_from = n)\n\n#By race\nsample_race &lt;- cps_org |&gt; \n  mutate(wbho = to_factor(wbho)) |&gt;\n  summarize(n=n(),\n            .by = c(wbho, year)) |&gt; \n  pivot_wider(id_cols = year, names_from = wbho, values_from = n)\n\n#By race and gender\nsample_race_gender &lt;- cps_org |&gt; \n  mutate(wbho = to_factor(wbho)) |&gt;\n  mutate(female = to_factor(female)) |&gt;\n  summarise(n=n(),\n        .by=c(wbho, female, year)) |&gt;\n  unite(col = \"demographic\", c(wbho, female), na.rm = TRUE) |&gt; \n  mutate(demographic = case_when(demographic == \"White_Female\" ~ \"White Women\", \n                                demographic == \"White_Male\" ~ \"White Men\",\n                                demographic == \"Black_Female\" ~ \"Black Women\", \n                                demographic == \"Black_Male\" ~ \"Black Men\",\n                                demographic == \"Hispanic_Female\" ~ \"Hispanic Women\", \n                                demographic == \"Hispanic_Male\" ~ \"Hispanic Men\",\n                                demographic == \"Other_Female\" ~ \"Other Women\", \n                                demographic == \"Other_Male\" ~ \"Other Men\")) |&gt;\n  pivot_wider(id_cols = year, names_from = demographic, values_from = n)\n\n#Join dataframes\nsample_sizes &lt;-  left_join(sample_gender, sample_race, by='year') |&gt;\n  left_join(sample_race_gender) |&gt; \n  select(year, starts_with(\"White\"), starts_with(\"Black\"), starts_with(\"Hispanic\"), Women, Men)\n\n#Save to Excel\nwb$add_worksheet(sheet = \"Sample sizes\") $\n  add_data(x = sample_sizes)",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#sheet-3-wages-by-educational-attainment",
    "href": "median_wages_demos.html#sheet-3-wages-by-educational-attainment",
    "title": "State-level median wages by demographic",
    "section": "Sheet 3: Wages by educational attainment",
    "text": "Sheet 3: Wages by educational attainment\nFinally, this code chunk generates median wages by educational attainment. The code uses the same weighted median wage methodology as above.\nIt also includes two sets of custom educational categories, which you can exclude or modify as needed. See the EPI Microdata variables list to learn how the gradeatn variable is categorized by default.\n\nFilter the data\nThe gradeatn variable is only available from 1992, so first we need to filter the data to exclude irrelevant years.\n\nnew_cps_org &lt;- filter(cps_org, year &gt;= 1992)\n\n\n\nCalculate wages by education level\nNow we’ll run the code. The first part of both sections of code here creates custom educational attainment categories. The second part calulcates wages.\n\n#Single category Associate degree\nwages_sing_assoc &lt;- new_cps_org |&gt; \n  mutate(educat = case_when(gradeatn %in% c(1,2,3,4,5,6,7,8) ~ \"No HS diploma\",\n                          gradeatn == 9 ~ \"HS graduate\",\n                          gradeatn == 10 ~ \"Some college but no degree\",\n                          gradeatn == 11 | gradeatn == 12 ~ \"Associate degree-all\",\n                          gradeatn %in% c(13,14,15,16) ~ \"Bachelor's degree or more\")) |&gt; \n  summarise(\n      wage_median = averaged_median(\n        x = realwage, \n        w = orgwgt/12,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(educat, year))\n\n#Two category Associate degree\nwages_two_assoc &lt;- new_cps_org |&gt; \n  mutate(educat = case_when(gradeatn %in% c(1,2,3,4,5,6,7,8) ~ \"No HS diploma\",\n                          gradeatn == 9 ~ \"HS graduate\",\n                          gradeatn == 10 ~ \"Some college but no degree\",\n                          gradeatn == 11 ~ \"Associate degree-occupational/vocational\",\n                          gradeatn == 12 ~ \"Associate degree-academic program\",\n                          gradeatn %in% c(13,14,15,16) ~ \"Bachelor's degree or more\")) |&gt; \n  summarise(\n      wage_median = averaged_median(\n        x = realwage, \n        w = orgwgt/12,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(educat, year))\n\n\n\nFormat and save output\nHere we’ll save our two dataframes to a new Excel sheet, formatting the output so both dataframes fit on a single sheet.\n\nwb$add_worksheet(sheet = \"Education median wages\") $\n  add_data(x = wages_sing_assoc, start_col = 1) $\n  add_data(x = wages_two_assoc, start_col = 8)",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "median_wages_demos.html#save-your-workbook",
    "href": "median_wages_demos.html#save-your-workbook",
    "title": "State-level median wages by demographic",
    "section": "Save your workbook",
    "text": "Save your workbook\nNote: Don’t forget to file names and file paths to match your setup before running this code chunk.\n\nwb_save(wb, \"output/wisconsin_wages_2025.xlsx\")\n\nAll done! Happy coding!",
    "crumbs": [
      "R code",
      "Code Repository",
      "State-level median wages by demographic"
    ]
  },
  {
    "objectID": "viz_workshop.html",
    "href": "viz_workshop.html",
    "title": "Visualizing Data",
    "section": "",
    "text": "This training was originally presented as a workshop at EARNCon 2023.\nIn this training you will learn how to create data visualizations using free tools like ggplot2 and Tableau.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#welcome-to-the-visualizing-data-training",
    "href": "viz_workshop.html#welcome-to-the-visualizing-data-training",
    "title": "Visualizing Data",
    "section": "",
    "text": "This training was originally presented as a workshop at EARNCon 2023.\nIn this training you will learn how to create data visualizations using free tools like ggplot2 and Tableau.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#introduction",
    "href": "viz_workshop.html#introduction",
    "title": "Visualizing Data",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n\n\n1.1 Brief overview of ggplot2\n\n\nggplot2 is a data visualization package for R that allows users to create complex plots in a structured manner. It’s based on the Grammar of Graphics, which provides a coherent system for describing and building graphics.\n\n\n1.2 Philosophy behind ggplot2\n\n\nAt the core of ggplot2 is the idea of layers. This means starting with a blank canvas and then iteratively adding layers to create the desired visualization.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#basic-chart-creation",
    "href": "viz_workshop.html#basic-chart-creation",
    "title": "Visualizing Data",
    "section": "2. Basic chart creation",
    "text": "2. Basic chart creation\n\n2.1. Basic syntax: The Big Three\n\n\n\nggplot()\naes()\ngeom_point()\n\nThe foundation of any ggplot2 visualization starts with the ggplot() function. Within ggplot(), we call aes() to designate aesthetic mappings, and then append geometries like geom_point() to visually represent data points.\n\n\n2.2. Practical example\n\n\nUsing the mtcars dataset, we’ll illustrate the relationship between a car’s horsepower (hp) and its fuel efficiency (mpg).\n\n#First thing first, install Tidyverse using\n\n# install.packages('tidyverse')\n# install.packages('highcharter')\n\n#Load tidyverse library\nlibrary(tidyverse)\nlibrary(here)\n\nmtcars &lt;- mtcars\n\nA simple scatter plot:\n\n# A simple scatter\nggplot(data = mtcars, aes(x=mpg, y=hp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nA simple histogram using geom_histogram()\n\nggplot(mtcars, aes(x=mpg)) + \n  geom_histogram(binwidth=3)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#adding-layers-and-customizations",
    "href": "viz_workshop.html#adding-layers-and-customizations",
    "title": "Visualizing Data",
    "section": "3. Adding layers and customizations",
    "text": "3. Adding layers and customizations\n\n\n\n# Load some data from Github. \n# Major industries, union density, real median wages, and employment. 2000 to 2022\nind_data &lt;- read.csv(url('https://raw.githubusercontent.com/Economic/earn_code_library/main/data/industry_union_wage_emp.csv'))\n\n# ind_data &lt;- read_csv(file = here('data/industry_union_wage_emp.csv'), col_names = TRUE)\n\n#Keep 2022 data\nind_data2022 &lt;- ind_data %&gt;%  \n  filter(year==2022)\n\nggplot(ind_data2022, aes(x=union_density, y=real_med_wage)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n3.1 Adding geometries\n\n\nAn insightful visualization often arises from combining various layers and customizing aesthetics.\nBuilding on our scatter plot from earlier, let’s include a smoothed line to better discern the relationship between mpg and hp:\n\nggplot(data=ind_data2022, aes(x=union_density, y=real_med_wage)) +\n  geom_point() +\n  # Add a smoothed line with customized aesthetics\n  geom_smooth(method = 'lm', se = FALSE, col = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe geom_smooth() with method=“lm” adds a linear regression line. The se=FALSE ensures the standard error bands are not plotted, and we’ve chosen a distinct red color for the line.\n\n\n3.2 Customizing Aesthetics\n\n\nA major advantage of ggplot2 is its flexibility in customizing visual properties of your plots.\nFor instance, modifying the scatter plot by adjusting point properties:\n\n#Bar chart with colors\nggplot(data=ind_data2022, aes(x=mind16, y=real_med_wage, fill=mind16))+\n  geom_col()\n\n\n\n\n\n\n\n#Bubble scatter chart\nggplot(ind_data2022, aes(x=union_density, y=real_med_wage, size=(total_emp/1000))) +\n  geom_point()\n\n\n\n\n\n\n\n#Bubble color scatter chart\nggplot(ind_data2022, aes(x=union_density, y=real_med_wage, color=mind16, size=(total_emp/1000))) + \n  geom_point()\n\n\n\n\n\n\n\n\nLayering and customization in ggplot2 ensures your visualizations are both visually appealing and insightful.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#customizing-plots-with-labels-and-themes",
    "href": "viz_workshop.html#customizing-plots-with-labels-and-themes",
    "title": "Visualizing Data",
    "section": "4. Customizing Plots with labels and themes",
    "text": "4. Customizing Plots with labels and themes\n\n4.1. Labeling and Titling\n\n\nLabeling is an integral part of making your plots interpretable. While some labels are inferred directly from the data, you often need to specify or customize them.\nHere’s how to add a title, x-axis label, and y-axis label to our scatter plot with the labs() function:\n\nggplot(data=ind_data, aes(x=year, y=union_density, color=mind16)) +\n  geom_line() +\n  \n  # labs() function to add labels\n  labs(title = 'The share of workers covered by a union contract has steadily decreased... for now',\n       subtitle = 'Union density by industry, 2000-2022',\n       x = 'Union density',\n       y = 'Real median wage',\n       color= 'Industry',\n       size = 'Total emp (1000s)',\n       caption = 'Note: Data refers to workers 16+. Union workers are those who \\n are members or covered by a union contract.\\nSource: EARN Analysis of CPS ORG data.')\n\n\n\n\n\n\n\n\n\n\n4.2. Adjusting Text Elements\n\n\nText elements such as titles, axis labels, and annotations can be modified to better fit your plot’s aesthetic or to match specific publication requirements.\nHere’s an example of adjusting the legend’s size and position\n\n#Example of our line plot\nggplot(data=ind_data, aes(x=year, y=union_density, color=mind16)) +\n  geom_line() +\n  # labs() function to add labels\n\n  labs(title = 'The share of workers covered by a union contract has steadily decreased... for now',\n       subtitle = 'Union density by industry, 2000-2022',\n       x = 'Union density',\n       y = 'Real median wage',\n       color= 'Industry',\n       size = 'Total emp (1000s)',\n       caption = 'Note: Data refers to workers 16+. Union workers are those who are members or covered by a union contract.\\nSource: EARN Analysis of CPS ORG data.') +\n  \n   #fix our ugly legend!\n  theme(legend.position = \"bottom\",\n        legend.box = \"vertical\",\n        legend.text = element_text(size = 9),\n        legend.title = element_text(size = 9),\n        # Edit plot caption\n        plot.caption = element_text(hjust = 0),\n        plot.title = element_text(size=12, color='maroon4', face='bold'))\n\n\n\n\n\n\n\n\n\n\n4.3. Themes in ggplot2\n\n\nggplot2 offers pre-set themes to modify plot aesthetics. Themes are a quick way to change the overall appearance of a plot, ensuring consistency presentations, papers, or reports.\nFor instance, let’s take the line chart we’ve been working with and apply a black and white theme:\n\n#Example of our line plot\nggplot(data=ind_data, aes(x=year, y=union_density, color=mind16)) +\n  geom_line() +\n  \n  #Note: Every subsequent theme() will supersede the previous. So be mindful!\n  theme_bw() +\n  # theme_dark() + \n  # theme_light() +\n  \n  # labs() function to add labels\n  labs(title = 'The share of workers covered by a union contract has steadily decreased... for now',\n       subtitle = 'Union density by industry, 2000-2022',\n       x = 'Union density',\n       y = 'Real median wage',\n       color= 'Industry',\n       size = 'Total emp (1000s)',\n       caption = 'Note: Data refers to workers 16+. Union workers are those who are members or covered by a union contract.\\nSource: EARN Analysis of CPS ORG data.') +\n  \n   #fix our ugly legend!\n  theme(legend.position = \"bottom\",\n        legend.box = \"vertical\",\n        legend.text = element_text(size = 9),\n        legend.title = element_text(size = 9),\n        # Edit plot caption\n        plot.caption = element_text(hjust = 0),\n        plot.title = element_text(size=12, color='maroon4', face='bold'))\n\n\n\n\n\n\n\n\nNow, let’s try using pre-made theme. ThemePark by Matthew B. Jane\n\ninstall.packages(\"remotes\")\n\nInstalling package into '/home/ecohn/R/x86_64-pc-linux-gnu-library/4.4'\n(as 'lib' is unspecified)\n\nremotes::install_github(\"MatthewBJane/ThemePark\")\n\nSkipping install of 'ThemePark' from a github remote, the SHA1 (30989dac) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(ThemePark)\n\nthemepark_themes\n\n              theme                     creator\n1            barbie             Matthew B. Jané\n2       oppenheimer Matthew B. Jané & Toki Liam\n3          starwars             Matthew B. Jané\n4             zelda               Alex Slavenko\n5        terminator               Alex Slavenko\n6         spiderman           Velu P.K. Immonen\n7            avatar           Velu P.K. Immonen\n8        gryffindor                Begum Ozemek\n9        hufflepuff                Begum Ozemek\n10        ravenclaw                Begum Ozemek\n11        slytherin                Begum Ozemek\n12         futurama             Tylor J. Harlow\n13         simpsons             Tylor J. Harlow\n14   lordoftherings                 Ethan Milne\n15    gameofthrones              Brennan Antone\n16        godfather      Francisco Garre-Frutos\n17             nemo        Christopher T. Kenny\n18          friends         Alexis van STRAATEN\n19            alien                Luke Pilling\n20   grand_budapest               Katya Kustova\n21    asteroid_city               Katya Kustova\n22  french_dispatch               Katya Kustova\n23 moonrise_kingdom               Katya Kustova\n24              elf        Christopher T. Kenny\n\n\n\nggplot(data=ind_data, aes(x=year, y=union_density, color=mind16)) +\n  geom_line() +\n  \n  #Theme\n  theme_minimal()+\n  \n  # labs() function to add labels\n  labs(title = 'The share of workers covered by a union contract has steadily decreased... for now',\n       subtitle = 'Union density by industry, 2000-2022',\n       x = 'Union density',\n       y = 'Real median wage',\n       color= 'Industry',\n       size = 'Total emp (1000s)',\n       caption = 'Note: Data refers to workers 16+. Union workers are those who are members or covered by a union contract.\\nSource: EARN Analysis of CPS ORG data.') +\n  \n   #fix our ugly legend!\n  theme(legend.position = \"bottom\",\n        legend.box = \"vertical\",\n        legend.text = element_text(size = 9),\n        legend.title = element_text(size = 9),\n        # Edit plot caption\n        plot.caption = element_text(hjust = 0, size = 8),\n        plot.title = element_text(size=12, color='maroon4', face='bold'))+\n  \n  theme_barbie()\n\n\n\n\n\n\n\n\n[Insert EARN/EPI/CGI example]?\nMastering these customization techniques will make your plots informative, engaging, and tailored for their intended audience!",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#exporting-your-plot",
    "href": "viz_workshop.html#exporting-your-plot",
    "title": "Visualizing Data",
    "section": "5. Exporting your plot",
    "text": "5. Exporting your plot\n\n\nNow to export your ggplot for the refrigerator\n\nfinal_plot &lt;- ggplot(data=ind_data, aes(x=year, y=union_density, color=mind16)) +\n  geom_line() +\n  \n  theme_minimal() +\n  \n  # labs() function to add labels\n  labs(title = 'The share of workers covered by a union contract has steadily decreased... for now',\n       subtitle = 'Union density by industry, 2000-2022',\n       x = 'Union density',\n       y = 'Real median wage',\n       color= 'Industry',\n       size = 'Total emp (1000s)',\n       caption = 'Note: Data refers to workers 16+. Union workers are those who are members or covered by a union contract.\\nSource: EARN Analysis of CPS ORG data.') +\n  \n   #fix our ugly legend!\n  theme(legend.position = \"bottom\",\n        legend.box = \"vertical\",\n        legend.text = element_text(size = 9),\n        legend.title = element_text(size = 9),\n        # Edit plot caption\n        plot.caption = element_text(hjust = 0, size = 8),\n        plot.title = element_text(size=12, color='maroon4', face='bold'))\n\n\n5.1 Saving your ggplot\n\n# Our chart object\nfinal_plot\n\n\n\n\n\n\n\nggsave(plot = final_plot,\n       #name of our chart\n       filename = 'final_union_industry_scatter.png',\n       # Save location for our chart\n       path = here('output/'), \n       # Dots per inch (300+ is considered high-res)\n       dpi = 300)\n\nSaving 7 x 5 in image",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#advanced-plot-types",
    "href": "viz_workshop.html#advanced-plot-types",
    "title": "Visualizing Data",
    "section": "6. Advanced Plot Types",
    "text": "6. Advanced Plot Types\n\n6.1. Faceting and Multi-panel Plots\n\n\nFaceting enables the creation of multi-panel plots, helping visualize patterns across different subgroups without generating individual plots for each subgroup.\nLet’s view scatter plots of mpg vs. hp but facet them by the number of cylinders:\n\nggplot(mtcars, aes(y=hp, x=mpg))+\n  geom_point()+\n  facet_wrap(~cyl)\n\n\n\n\n\n\n\nind_data_years &lt;- ind_data %&gt;% \n  filter(year %in% c(2000, 2008, 2022))\n\nggplot(ind_data_years, aes(x=union_density, y=real_med_wage)) + \n  geom_point() + \n  facet_wrap(~year) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n6.3. Interactive charts\n\n#install.packages('highcharter')\nlibrary(highcharter)\n\n\n#Line chart with colors\n\nlinechart &lt;- highchart() %&gt;%\n  \n  hc_add_series(data = ind_data, hcaes(x = year, y = union_density, group = mind16), \n                type = 'line')\n\nlinechart\n\n\n\n\n\nAdvanced plot types and features will elevate your data visualization skills, allowing you to craft detailed and insightful plots tailored to diverse datasets and questions.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "viz_workshop.html#resources-for-layer-based-chart-visualization",
    "href": "viz_workshop.html#resources-for-layer-based-chart-visualization",
    "title": "Visualizing Data",
    "section": "7.0 Resources for layer-based chart visualization!",
    "text": "7.0 Resources for layer-based chart visualization!\n\nOfficial Tidyverse page for ggplot2 https://ggplot2.tidyverse.org/index.html\nR Graphics Cookbook\nHighcharter package for interactive charts",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Visualizing Data"
    ]
  },
  {
    "objectID": "rolling_averages.html",
    "href": "rolling_averages.html",
    "title": "Calculating rolling averages",
    "section": "",
    "text": "This script will teach you how to calculate simple rolling averages using employment-to-population ratios (EPOPs). Note: this code will be updated soon to account for missing October 2025 CPS data.",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculating rolling averages"
    ]
  },
  {
    "objectID": "rolling_averages.html#load-required-libraries",
    "href": "rolling_averages.html#load-required-libraries",
    "title": "Calculating rolling averages",
    "section": "Load required libraries",
    "text": "Load required libraries\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code. If you haven’t yet set up your computer to run EPI CPS microdata extracts, complete that process before running this code.\n\n#Load necessary libraries\nlibrary(tidyverse) #this package contains most common R functions\nlibrary(epiextractr) #allows you to get CPS (e.g., Basic, ORG) extracts\nlibrary(labelled) #allows you to easily clean data; includes functions like 'to_factor'\nlibrary(zoo) #contains functions like rollmean and rollsum\nlibrary(openxlsx2) #allows you to format and export data to an Excel workbook",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculating rolling averages"
    ]
  },
  {
    "objectID": "rolling_averages.html#import-and-clean-data",
    "href": "rolling_averages.html#import-and-clean-data",
    "title": "Calculating rolling averages",
    "section": "Import and clean data",
    "text": "Import and clean data\nNote: Don’t forget to update years to match your setup before running the script.\nRunning this script chunk will call the BLS Current Population Survey Basic data required to calculate rolling average EPOPs. It will also create a variable that will allow you to calculate prime-age EPOPs and set up some date formatting for the rolling average calculation later.\n\n# Import CPS ORG data\nbasic &lt;- load_basic(2018:2024, \"year\", \"month\", \"basicwgt\", \"age\", \"wbhao\", \"selfinc\", \"emp\") |&gt;\n  #Filter to working age population with employment information\n  filter(age &gt;= 16, !is.na(emp)) |&gt; \n  # Create prime-age variable\n  mutate(prime_age = case_when(between(age, 25, 54) ~ 1, TRUE ~ 0)) |&gt; \n  # Create date in yyyy-mm-dd format\n  mutate(date = ymd(paste0(year,'-', month,'-1')))",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculating rolling averages"
    ]
  },
  {
    "objectID": "rolling_averages.html#calculate-monthly-rolling-averages-for-epops",
    "href": "rolling_averages.html#calculate-monthly-rolling-averages-for-epops",
    "title": "Calculating rolling averages",
    "section": "Calculate monthly rolling averages for EPOPs",
    "text": "Calculate monthly rolling averages for EPOPs\nThis code chunk calculates the (prime-age) EPOP by finding a weighted average of the binary variable emp (1 = employed, 0 = unemployed or not in the labor force), sets up the rolling average, and applies an (optional) data suppression threshold. The suppression threshold is likely not necessary for national data, but it can become important at the state level or with narrow demographic cuts. Though there is no hard and fast rule for how large a sample needs to be, smaller samples will produce more noisy, less reliable data.\n\nepop_all &lt;- basic |&gt;\n  # Optional: filter to calculate prime-age EPOPS\n  filter(prime_age == 1) |&gt; \n  # Calculate EPOP as the weighted mean of 'emp' variable\n  summarise(epop = weighted.mean(emp, w = basicwgt),\n            n=n(),\n            .by=date) |&gt; \n# Calculate rolling average of previous 12 months for both EPOP and sample size\n  mutate(all_epop12 = rollmean(epop, k = 12, align = \"right\", fill = NA),\n         all_sample12 = rollsum(n, k = 12, align = \"right\", fill = NA)) |&gt; \n  select(date, all_epop12, all_sample12) |&gt;\n  # Suppress data with sample size of &lt; 250\n  mutate(all_epop12 = if_else(is.na(all_sample12) | all_sample12 &lt; 250,\n                            NA_real_, all_epop12))\n\n\nCalculate EPOPs by race/ethnicity and format output\nThis section adds code that breaks out EPOPs by race/ethnicity, using the wbhao variable. It also pivots the data so that each race/ethnicity group is its own column.\n\nepop_race &lt;- basic |&gt;\n  summarise(epop = weighted.mean(emp, w = basicwgt),\n            n=n(),\n            .by=c(wbhao, date)) |&gt; \n  mutate(race = to_factor(wbhao)) |&gt; \n  # Ensure correct order within each racial/ethnic category\n  arrange(race, date) |&gt;  \n  # Compute rolling mean per 'race' group\n  group_by(race) |&gt;       \n  mutate(epop12 = rollmean(epop, k = 12, align = \"right\", fill = NA),\n         sample12 = rollsum(n, k = 12, align = \"right\", fill = NA)) |&gt; \n  ungroup() |&gt; \n  select(date, race, epop12, sample12) |&gt;\n  # Pivot the data to display each race/ethnicity as its own column with appropriate label\n  pivot_wider(\n    names_from = race,\n    values_from = c(epop12, sample12),\n    names_glue = \"{race}_{.value}\"\n  )",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculating rolling averages"
    ]
  },
  {
    "objectID": "rolling_averages.html#export-your-data-to-a-downloadable-excel",
    "href": "rolling_averages.html#export-your-data-to-a-downloadable-excel",
    "title": "Calculating rolling averages",
    "section": "Export your data to a downloadable Excel",
    "text": "Export your data to a downloadable Excel\nUse openxlsx2 to create a downloadable Excel. Don’t forget to edit the file path to ensure it saves to the right location.\n\n#  Export epop tables to one workbook \nwb &lt;- wb_workbook()\n\nwb$ \n  # Add worksheet\n  add_worksheet(sheet = \"US EPOPs\")$\n  add_data(x = epop_all)$\n  add_worksheet(sheet = \"EPOPs by race\")$\n  add_data(x = epop_race)\n\nwb_save(wb, file = \"output/epop_tables.xlsx\", overwrite = TRUE)\n\nAnd that’s it! You can benchmark your data to the State of Working America Data Library before filtering to specific states or demographic cuts to ensure reliability. As always, be sure to keep an eye on those sample sizes!",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculating rolling averages"
    ]
  },
  {
    "objectID": "stata_binipolate.html",
    "href": "stata_binipolate.html",
    "title": "Binipolate for Stata",
    "section": "",
    "text": "Binipolate is a Stata function to bin data and linearly interpolate percentiles. See the binipolate github page for more detail.\n\n\nnet install binipolate, from(\"https://raw.githubusercontent.com/Economic/binipolate/master/\")\n\n\n\nIf you use binipolate, please cite it:\n\nZipperer, Ben and Zane Mokhiber. 2020. binipolate: A Stata function to bin data and linearly interpolate percentiles. https://github.com/Economic/binipolate",
    "crumbs": [
      "Code Packages",
      "Stata packages"
    ]
  },
  {
    "objectID": "stata_binipolate.html#installation",
    "href": "stata_binipolate.html#installation",
    "title": "Binipolate for Stata",
    "section": "",
    "text": "net install binipolate, from(\"https://raw.githubusercontent.com/Economic/binipolate/master/\")",
    "crumbs": [
      "Code Packages",
      "Stata packages"
    ]
  },
  {
    "objectID": "stata_binipolate.html#citations",
    "href": "stata_binipolate.html#citations",
    "title": "Binipolate for Stata",
    "section": "",
    "text": "If you use binipolate, please cite it:\n\nZipperer, Ben and Zane Mokhiber. 2020. binipolate: A Stata function to bin data and linearly interpolate percentiles. https://github.com/Economic/binipolate",
    "crumbs": [
      "Code Packages",
      "Stata packages"
    ]
  },
  {
    "objectID": "foreign_born_stats_ipumsr.html",
    "href": "foreign_born_stats_ipumsr.html",
    "title": "Using ipumsr to load immigrant statistics",
    "section": "",
    "text": "The following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(ipumsr)\nlibrary(janitor)\nlibrary(labelled)\nlibrary(fs)",
    "crumbs": [
      "R code",
      "Code Repository",
      "Using ipumsr to load immigrant statistics"
    ]
  },
  {
    "objectID": "foreign_born_stats_ipumsr.html#create-and-clean-an-extract-using-ipumsr.",
    "href": "foreign_born_stats_ipumsr.html#create-and-clean-an-extract-using-ipumsr.",
    "title": "Using ipumsr to load immigrant statistics",
    "section": "Create and clean an extract using IPUMSR.",
    "text": "Create and clean an extract using IPUMSR.\nYou must set up an IPUMS API key before using the ipumsr package. For instructions on how to set up the IPUMS API, see “Introduction to the IPUMS API for R Users.”\nFor guidance on how to define an extract, see “Microdata API Requests.” You can view the list available IPUMS ACS samples and their IDs on the IPUMS sample IDs page.\nNote: You only need to run define_extract_micro() once, unless your extract parameters (e.g., years or variables) change. This command triggers an IPUMS API call, which may take several minutes depending on the extract size. If running this as part of a larger script, comment out this command to avoid repeated downloads.\n\n# Load samples\nacs_samps &lt;- ipumsr::get_sample_info('usa')\n\n# Create a vector of sample IDs to load\nyears &lt;- c('us2000a', 'us2001a', 'us2002a', 'us2003a', 'us2004a', 'us2005a',\n           'us2006a', 'us2007a', 'us2008a', 'us2009a', 'us2010a', 'us2011a',\n           'us2012a', 'us2013a', 'us2014a', 'us2015a', 'us2016a', 'us2017a',\n           'us2018a', 'us2019a', 'us2020a', 'us2021a', 'us2022a', 'us2023a')\n\nacs_extr &lt;- define_extract_micro(\n  \"usa\",\n  description = 'ACS extract for Immigration statistics',\n  samples = years,\n  # Select the variables to load\n  variables = list('STATEFIP','COUNTYFIP', 'SEX', 'AGE', 'RACE', 'HISPAN',\n                   'BPL', 'CITIZEN', 'YRNATUR', 'YRIMMIG', 'YRSUSA1',\n                   'LANGUAGE', 'EMPSTAT', 'LABFORCE', 'OCC', 'IND')) |&gt; \n  submit_extract() |&gt; \n  wait_for_extract()\n\n# Download extract to input folder\ndl_extr &lt;- download_extract(extract = acs_extr,\n                                      download_dir = 'input/',\n                                      overwrite = TRUE)\n\nLoad the extract (the xml file) and clean it up before conducting analysis.\nNote: Your extract will likely have a different file name, double-check this and update the script accordingly before running the following chunk.\n\n# NOTE: Your project directory and xml file may look different!\nacs_raw &lt;- read_ipums_micro(ddi = 'input/usa_00010.xml')\n\nacs &lt;- acs_raw |&gt; \n  # Use the janitor library to clean up names\n  janitor::clean_names() |&gt; \n  # Use labelled library to create custom value labels\n  # relabel citizen=0 to \"Not foreign born\" per https://usa.ipums.org/usa-action/variables/CITIZEN#comparability_section\n  labelled::set_value_labels(citizen = c('Not foreign born'=0, 'Born abroad of American parents'=1, 'Naturalized citizen'=2, 'Not a citizen'=3)) |&gt; \n  mutate(nativity = case_when(citizen %in% c(0,1) ~ 1,\n                              citizen %in% c(2,3) ~ 2)) |&gt; \n  add_value_labels(nativity = c('Native' = 1, 'Foreign-born'=2))",
    "crumbs": [
      "R code",
      "Code Repository",
      "Using ipumsr to load immigrant statistics"
    ]
  },
  {
    "objectID": "foreign_born_stats_ipumsr.html#benchmark-your-data",
    "href": "foreign_born_stats_ipumsr.html#benchmark-your-data",
    "title": "Using ipumsr to load immigrant statistics",
    "section": "Benchmark your data",
    "text": "Benchmark your data\nRun a US population benchmark using the Census ACS table statistics to check your data before continuing.\n\n# Do your US population estimates benchmark with the Census ACS table statistics?\n#   https://data.census.gov/table/ACSDP1Y2023.DP05?q=DP05:+ACS+Demographic+and+Housing+Estimates\n\nus_pop &lt;- acs |&gt; \nsummarize(pop = sum(perwt, na.rm=TRUE),\n          .by=year)\n\nus_pop\n\n# A tibble: 24 × 2\n    year       pop\n   &lt;int&gt;     &lt;dbl&gt;\n 1  2000 281421906\n 2  2001 277075792\n 3  2002 280717370\n 4  2003 283051379\n 5  2004 285674993\n 6  2005 288398819\n 7  2006 299398485\n 8  2007 301621159\n 9  2008 304059728\n10  2009 307006556\n# ℹ 14 more rows",
    "crumbs": [
      "R code",
      "Code Repository",
      "Using ipumsr to load immigrant statistics"
    ]
  },
  {
    "objectID": "foreign_born_stats_ipumsr.html#run-your-analysis",
    "href": "foreign_born_stats_ipumsr.html#run-your-analysis",
    "title": "Using ipumsr to load immigrant statistics",
    "section": "Run your analysis!",
    "text": "Run your analysis!\nHere you will run your analysis to find three statistics:\n\nPopulation by citizenship status and year\nPopulation count and share by nativity and year\nEmployment counts and shares of immigrant workers by industry (See a list of industry codes and their associated titles here)\n\nDon’t forget to update the code to match your selection of years.\nThis code can also be easily altered to filter for specific groups. For example, you can filter by state, specific industry, or for prime-age workers. See the commented-out commands for examples. Be sure to check for viable sample sizes when using a smaller data set.\n\n# Population by citizenship status and year 2000–2023\nforeign_born_total &lt;- acs |&gt; \n  ## filter to just North Carolina\n  # filter(statefip == 37) |&gt;\n  mutate(citizen = to_factor(citizen)) |&gt; \n  summarize(pop = sum(perwt, na.rm=TRUE),\n            .by=c(year, citizen)) |&gt; \n  pivot_wider(id_cols = year, names_from = citizen, values_from = pop)\n\nforeign_born_total\n\n# A tibble: 24 × 5\n    year `Not foreign born` `Not a citizen` `Naturalized citizen`\n   &lt;int&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n 1  2000          248371297        18599549              12533932\n 2  2001          243578016        18804708              12743420\n 3  2002          245639288        19565399              13530751\n 4  2003          247351604        19749916              13917762\n 5  2004          249429239        19857656              14400045\n 6  2005          250569155        20836032              14933571\n 7  2006          259759423        21696303              15773084\n 8  2007          261446898        21843559              16204897\n 9  2008          263619759        21685745              16330357\n10  2009          266132797        21640993              16811829\n# ℹ 14 more rows\n# ℹ 1 more variable: `Born abroad of American parents` &lt;dbl&gt;\n\n# Population by nativity and year 2000–2023\nnativity &lt;- acs |&gt; \n  mutate(nativity = to_factor(nativity)) |&gt; \n  summarize(pop = sum(perwt, na.rm=TRUE),\n            .by=c(year, nativity)) |&gt;\n  mutate(share = pop/sum(pop), .by=year) |&gt; \n  pivot_wider(id_cols = year, names_from = nativity, values_from = c(pop, share))\n\nnativity\n\n# A tibble: 24 × 5\n    year pop_Native `pop_Foreign-born` share_Native `share_Foreign-born`\n   &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1  2000  250288425           31133481        0.889                0.111\n 2  2001  245527664           31548128        0.886                0.114\n 3  2002  247621220           33096150        0.882                0.118\n 4  2003  249383701           33667678        0.881                0.119\n 5  2004  251417292           34257701        0.880                0.120\n 6  2005  252629216           35769603        0.876                0.124\n 7  2006  261929098           37469387        0.875                0.125\n 8  2007  263572703           38048456        0.874                0.126\n 9  2008  266043626           38016102        0.875                0.125\n10  2009  268553734           38452822        0.875                0.125\n# ℹ 14 more rows\n\n# Industries and occupations of immigrant workers\n# This analysis pools 5 years of data\nnativity_ind &lt;- acs |&gt; \n  filter(year %in% c(2019:2023), age&gt;=16, empstat==1) |&gt; \n  ## filter for prime-age EPOP\n  # filter(age &gt;= 25 & age &lt;= 54) |&gt;\n  mutate(nativity = to_factor(nativity)) |&gt; \n  # Adjust perwt, dividing it by 5.\n  summarize(total_emp = sum(empstat * perwt/5, na.rm=TRUE),\n            n=n(),\n            .by=c(nativity, ind)) |&gt;\n  mutate(share = total_emp/sum(total_emp), .by=nativity) |&gt; \n  pivot_wider(id_cols = ind, names_from = nativity, values_from = c(total_emp, share, n))\n\nnativity_ind\n\n# A tibble: 302 × 7\n   ind       total_emp_Native `total_emp_Foreign-born` share_Native\n   &lt;int+lbl&gt;            &lt;dbl&gt;                    &lt;dbl&gt;        &lt;dbl&gt;\n 1 9160              1018174.                  128856.     0.00767 \n 2 5170               682919                   139648.     0.00514 \n 3 7870              3681723.                  803696.     0.0277  \n 4 5570               100653                    15650.     0.000758\n 5 6480               135431                    17229.     0.00102 \n 6 3291               667545                   118284      0.00503 \n 7 8680              7329304.                 1796732.     0.0552  \n 8 9670               401908                    34028.     0.00303 \n 9 5391              1614827.                  217887.     0.0122  \n10 5490               176175                    27847      0.00133 \n# ℹ 292 more rows\n# ℹ 3 more variables: `share_Foreign-born` &lt;dbl&gt;, n_Native &lt;int&gt;,\n#   `n_Foreign-born` &lt;int&gt;\n\n# See a list of industry codes and their associated titles here: https://usa.ipums.org/usa/volii/ind2022.shtml\n\nHappy coding!",
    "crumbs": [
      "R code",
      "Code Repository",
      "Using ipumsr to load immigrant statistics"
    ]
  },
  {
    "objectID": "prev_bootcamps.html",
    "href": "prev_bootcamps.html",
    "title": "EARNCon data bootcamp files",
    "section": "",
    "text": "EPI Bootcamp Website",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#data-analysis-in-r-first-steps",
    "href": "prev_bootcamps.html#data-analysis-in-r-first-steps",
    "title": "EARNCon data bootcamp files",
    "section": "Data analysis in R: First steps",
    "text": "Data analysis in R: First steps\nFirst steps to data analysis in R slides (pdf)\nEPI national wage percentiles data (csv)\n2022 EPI CPS ORG extracts data (zipped .dta)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#using-apis-to-access-data-in-r",
    "href": "prev_bootcamps.html#using-apis-to-access-data-in-r",
    "title": "EARNCon data bootcamp files",
    "section": "Using APIs to access data in R",
    "text": "Using APIs to access data in R\nAccessing BLS and Census data (code)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#data-analysis-in-r-video",
    "href": "prev_bootcamps.html#data-analysis-in-r-video",
    "title": "EARNCon data bootcamp files",
    "section": "Data analysis in R (Video)",
    "text": "Data analysis in R (Video)\nData analysis in R slides\nHistorical minimum wage data\nCPI-U-RS data\nEPI CPS ORG data",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#code-library-presentation-video",
    "href": "prev_bootcamps.html#code-library-presentation-video",
    "title": "EARNCon data bootcamp files",
    "section": "Code Library Presentation (Video)",
    "text": "Code Library Presentation (Video)\nCode library powerpoint",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#data-analysis-in-stata-video",
    "href": "prev_bootcamps.html#data-analysis-in-stata-video",
    "title": "EARNCon data bootcamp files",
    "section": "Data analysis in Stata (Video)",
    "text": "Data analysis in Stata (Video)\nData analysis in Stata slides\nStata session do file",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#first-steps-in-stata-video",
    "href": "prev_bootcamps.html#first-steps-in-stata-video",
    "title": "EARNCon data bootcamp files",
    "section": "2021 First steps in Stata (Video)",
    "text": "2021 First steps in Stata (Video)\nStata session 1 slides\nStata session 1 do file\nEPI CPS ORG 2020 stata data file",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#using-stata-effectively-video",
    "href": "prev_bootcamps.html#using-stata-effectively-video",
    "title": "EARNCon data bootcamp files",
    "section": "2021 Using Stata effectively (Video)",
    "text": "2021 Using Stata effectively (Video)\nStata session 2 slides\nStata session 2 do file\nCPI-U-RS data\nEPI CPS ORG 2020 stata data file\nFull EPI CPS ORG (zipped data files)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#first-steps-in-r-video",
    "href": "prev_bootcamps.html#first-steps-in-r-video",
    "title": "EARNCon data bootcamp files",
    "section": "2021 First steps in R (Video)",
    "text": "2021 First steps in R (Video)\nSlides\nacs_2019.dta\nepi_wage_percentiles.csv\nacs_wage_analysis.R",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "prev_bootcamps.html#using-r-effectively-video",
    "href": "prev_bootcamps.html#using-r-effectively-video",
    "title": "EARNCon data bootcamp files",
    "section": "2021 Using R effectively (Video)",
    "text": "2021 Using R effectively (Video)\nSlides\nacs_2019.dta\ngeocorr2018.csv\nacs_wage_analysis_LAST_TIME.R\nacs_wage_analysis_FINAL.R",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "EARNCon data bootcamp files"
    ]
  },
  {
    "objectID": "blsR_example.html",
    "href": "blsR_example.html",
    "title": "Downloading BLS data via blsR",
    "section": "",
    "text": "blsR is the R package written and maintained by the Bureau of Labor Statistics to provide users with BLS data, including Current Employment Statistics (CES), Worker characteristics data (CPS), inflation & prices (CPI), and Job Openings Layoff and Turnover Survey (JOLTS), among others.\nBLS provides this data through the use of an Application Programming Interface (API), which allows R to directly import data from BLS using the series IDs which are extensively documented by BLS. This process automates the manual retrieval of data using the BLS series report and delivers the tidied data.\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n\n# download blsR\n#install.library(\"blsR\")\n\n# import relevant libraries\nlibrary(tidyverse)\nlibrary(blsR)\nlibrary(here)\n\nIn order to access BLS data you must register for a unique API key. Save your key as an environmental object to be used later.\n\n# set key for BLS api\n#note: each user must register for unique BLS API key here: https://www.bls.gov/developers/home.htm\nbls_key &lt;- Sys.getenv(\"your-key-goes-here\")",
    "crumbs": [
      "R code",
      "Downloading BLS data via blsR"
    ]
  },
  {
    "objectID": "blsR_example.html#loading-blsr-and-setting-api-key",
    "href": "blsR_example.html#loading-blsr-and-setting-api-key",
    "title": "Downloading BLS data via blsR",
    "section": "",
    "text": "blsR is the R package written and maintained by the Bureau of Labor Statistics to provide users with BLS data, including Current Employment Statistics (CES), Worker characteristics data (CPS), inflation & prices (CPI), and Job Openings Layoff and Turnover Survey (JOLTS), among others.\nBLS provides this data through the use of an Application Programming Interface (API), which allows R to directly import data from BLS using the series IDs which are extensively documented by BLS. This process automates the manual retrieval of data using the BLS series report and delivers the tidied data.\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n\n# download blsR\n#install.library(\"blsR\")\n\n# import relevant libraries\nlibrary(tidyverse)\nlibrary(blsR)\nlibrary(here)\n\nIn order to access BLS data you must register for a unique API key. Save your key as an environmental object to be used later.\n\n# set key for BLS api\n#note: each user must register for unique BLS API key here: https://www.bls.gov/developers/home.htm\nbls_key &lt;- Sys.getenv(\"your-key-goes-here\")",
    "crumbs": [
      "R code",
      "Downloading BLS data via blsR"
    ]
  },
  {
    "objectID": "blsR_example.html#nominal-average-hourly-earnings-of-production-and-non-supervisory-employees",
    "href": "blsR_example.html#nominal-average-hourly-earnings-of-production-and-non-supervisory-employees",
    "title": "Downloading BLS data via blsR",
    "section": "Nominal average hourly earnings of production and non-supervisory employees",
    "text": "Nominal average hourly earnings of production and non-supervisory employees\nYou can import a single table using the blsR::get_n_series_table() function:\n\n## use blsR to pull in nominal wages\nnominal_wages &lt;- get_n_series_table(series = \"CEU0500000008\", api_key = bls_key, \n                                    start_year = 1965, end_year = 2023, \n                                    tidy = TRUE, annualaverage = TRUE) %&gt;% \n  # filter for annual data\n  filter(month == 13)",
    "crumbs": [
      "R code",
      "Downloading BLS data via blsR"
    ]
  },
  {
    "objectID": "blsR_example.html#real-average-hourly-earnings-of-production-and-non-supervsory-employees",
    "href": "blsR_example.html#real-average-hourly-earnings-of-production-and-non-supervsory-employees",
    "title": "Downloading BLS data via blsR",
    "section": "Real average hourly earnings of production and non-supervsory employees",
    "text": "Real average hourly earnings of production and non-supervsory employees\nYou can also import multiple series at once. For example, we can import AHE and CPI in order to calculate real wages.\n\n# set cpi codes\ncpi_codes &lt;- c(\"CUUR0000SA0\",\n               \"CEU0500000008\")\n\n# set cpi base\n#note: used to set base year for inflation-adjustment\ncpi_base &lt;- get_n_series_table(series_ids = \"CUUR0000SA0\", start_year = 2023, end_year = 2023, \n                               api_key = bls_key, tidy = TRUE, annualaverage = TRUE) %&gt;% \n  # filter annual 2023 data and pull CPI value as base\n  filter(month == 13) %&gt;% pull(CUUR0000SA0)\n\n# use blsR to pull CPI data\ncpi_output &lt;- get_n_series_table(series_ids = cpi_codes, start_year = 1947, end_year = 2023, \n                                 api_key = bls_key, tidy = TRUE, annualaverage = TRUE) %&gt;% \n  # filter annual data\n  filter(month == 13) %&gt;%\n  # rename for easier handling\n  rename(ahe = CEU0500000008, cpi = CUUR0000SA0) %&gt;% \n  # calculate real wages\n  mutate(ahe_real = ahe * (cpi_base/cpi)) %&gt;% \n  # export to delimited file\n  write.csv(here(\"ahe_real.csv\"))",
    "crumbs": [
      "R code",
      "Downloading BLS data via blsR"
    ]
  },
  {
    "objectID": "blsR_example.html#pulling-more-than-50-series",
    "href": "blsR_example.html#pulling-more-than-50-series",
    "title": "Downloading BLS data via blsR",
    "section": "Pulling more than 50 series",
    "text": "Pulling more than 50 series\nThe BLS API limits each call to 50 series, which can be limiting when you are trying to pull in large datasets. Here is a trick I use in the code that runs our Jobs and Unemployment page to workaround this limitation:\n\n# read in bls series codes\n#note: this is a random selection of codes used in our Jobs and Unemployment figures\nbls_codes &lt;- read.csv(here(\"bls_codes.csv\"))\n# remove any blanks\nbls_codes &lt;- bls_codes$series_id[bls_codes$series_id != \"\"]\n\n\n# use map to iteratively call blsR api at max number of series id\n#note: BLS restricts to max 50 series in a single call\njobs_day_df &lt;- map(split(bls_codes, ceiling(seq_along(bls_codes) / 50)), # split codes into groups of 50\n             # call blsR using series ids sliced into groups of 50\n             ~ get_n_series_table(series_ids = .x, start_year = 1939, end_year = 2023, \n                                  api_key = bls_key, tidy = TRUE)) %&gt;% \n  # map returns list, flatten by joining data\n  reduce(., function(df1, df2) full_join(df1, df2, by = c(\"year\", \"month\"))) %&gt;%\n  # define date\n  mutate(date = as.POSIXct(paste(year,month,1, sep = \"-\")),\n         date = as.Date(date)) %&gt;% \n  write_csv(here(\"jobs_day_example.csv\"))",
    "crumbs": [
      "R code",
      "Downloading BLS data via blsR"
    ]
  },
  {
    "objectID": "benchmarking_workshop.html",
    "href": "benchmarking_workshop.html",
    "title": "Benchmarking Data",
    "section": "",
    "text": "This training was originally presented as a workshop at EARNCon 2023.\nBenchmarking PowerPoint slides\n\nPublication: Older workers were devastated by the pandemic downturn and continue to face adverse employment outcomes\n\n\nimport packages\n\n# import packages\nlibrary(tidyverse)\nlibrary(epiextractr)\nlibrary(here)\n\n\n\nDefine CPS years\n\ncps_years &lt;- 2007:2020\n\n\ncps_vars &lt;- c(\"year\", \"month\", \"age\", \"female\", \"emp\", \"basicwgt\")\n\n\n# import basic CPS data\ncps_data &lt;- load_basic(cps_years, all_of(cps_vars)) %&gt;% \n  # restrict to 16+\n  filter(age &gt;= 16) %&gt;% \n  # create age categories\n  mutate(age_group = case_when(\n    age &lt;= 24 ~ \"16–24 years old\",\n    age &gt;= 25 & age &lt;= 54 ~ \"25–54 years old\",\n    age &gt;= 55 & age &lt;= 64 ~ \"55–64 years old\",\n    age &gt;= 65 ~ \"65+ years old\"),\n    # adjust weight\n    wgt = basicwgt/12)\n\nUsing EPI CPS Basic Monthly Extracts, Version 2025.9.11\n\n\nhttps://www.bls.gov/cps/aa2020/cpsaat18b.htm\n\ncps_data %&gt;% \n  # restrict to employed\n  filter(emp == 1, year == 2020) %&gt;% \n  # weighted employment count by year\n  group_by(year) %&gt;% tally(wt = wgt) %&gt;% mutate(n = n/1000)\n\n# A tibble: 1 × 2\n   year       n\n  &lt;int&gt;   &lt;dbl&gt;\n1  2020 147795.\n\n\n\n# age category benchmark: https://www.bls.gov/cps/aa2020/cpsaat03.htm\ncps_data %&gt;% \n  # restrict to employed\n  filter(emp == 1, year == 2020) %&gt;% \n  # weighted employment count by year\n  group_by(year, age_group) %&gt;% tally(wt = wgt) %&gt;% mutate(n = n/1000) %&gt;% \n  # reshape data\n  pivot_wider(id_cols = year, names_from = age_group, values_from = n)\n\n# A tibble: 1 × 5\n# Groups:   year [1]\n   year `16–24 years old` `25–54 years old` `55–64 years old` `65+ years old`\n  &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1  2020            17192.            95310.            25469.           9824.\n\n\n\n\nFigure C: change in employment-to-population ratio across age categories\n\nemp &lt;- cps_data %&gt;%\n  # isolate recession beginning and end years\n  filter(!is.na(emp),\n      year %in% c(2007, 2011, 2019, 2020)) %&gt;% \n  # weighted employment * population count by year and age group\n  group_by(year, age_group) %&gt;% summarise(emp = sum(emp * wgt), pop = sum(wgt)) %&gt;% \n  # calculate EPOPs\n  mutate(epop = emp/pop) %&gt;% pivot_wider(id_cols = age_group, names_from = year, values_from = epop) %&gt;% \n  # calculate percent change in EPOPs\n  transmute(age_group = age_group, `2007–2011` = `2011`-`2007`, `2019–2020` = `2020`-`2019`) %&gt;%\n  # output to output folder\n  write_csv(here(\"output/older_worker_epop.csv\"))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\nFigure D: Change in employment-population ratios during the Great Recession and the COVID-19 recession, 2007–2011 and 2019–2020, by gender and older age group\n\nepop_gender &lt;- cps_data %&gt;% \n  # isolate recession beginning and end years\n  filter(!is.na(emp),\n      year %in% c(2007, 2011, 2019, 2020)) %&gt;% \n  # assign value of female as character label (\"Female\" instead of 1 and \"Male\" instead of 0)\n  #note: requires the installation of haven\n  #note: using double \":\" is a way of referencing functions in a package without importing entire package\n  mutate(female = as.character(haven::as_factor(female))) %&gt;% \n  # weighted employment * population count by year and age group AND female\n  group_by(year, age_group, female) %&gt;% summarise(emp = sum(emp * wgt), pop = sum(wgt)) %&gt;% \n  # calculate epops\n  mutate(epop = emp/pop) %&gt;% ungroup() %&gt;% \n  # reshape wider to calculate change in EPOPs over recession time perios\n  pivot_wider(id_cols = c(\"age_group\", \"female\"), names_from = year, values_from = epop) %&gt;% \n  # calculate percent change in EPOPs\n  transmute(age_group = age_group, female = female, \n            `2007-2011` = `2011` - `2007`, `2019-2020` = `2020` - `2019`) %&gt;% \n  # reshape data longer for ggplot2\n  #note: change in epop by age group will group bars by recession time period, create panels by gender,\n  #       reshape long for each dimension\n  pivot_longer(cols = c(`2007-2011`, `2019-2020`), names_to = \"recession\", values_to = \"epop_change\")\n\n`summarise()` has grouped output by 'year', 'age_group'. You can override using\nthe `.groups` argument.\n\n\n\n\nCreate mock of Figure D using ggplot2\n\n# change in epop by age group and recession time periods\nggplot(epop_gender, aes(x = age_group, y = epop_change, fill = recession)) +\n  # use position \"dodge\" to group bars instead of stacking\n  geom_col(position = \"dodge\") +\n  # use to create side-by-side panels for easier viewing\n  facet_wrap(~female)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Benchmarking Data"
    ]
  },
  {
    "objectID": "epi_libraries.html",
    "href": "epi_libraries.html",
    "title": "EPI packages for R",
    "section": "",
    "text": "Installation instructions are available on each package’s Github page.\nrealtalk\n\nrealtalk makes it easy to load and use common US price indexes in R.\n\nepidatatools\n\nepidatatools contains functions we find useful at EPI that don’t have exact analogues elsewhere in the R package ecosystem.\n\nepiextractr\n\nuse epiextractr to download CPS data from the EPI microdata site.\n\n\n\n\n Back to top",
    "crumbs": [
      "Code Packages",
      "R Packages"
    ]
  },
  {
    "objectID": "tidycensus_example.html",
    "href": "tidycensus_example.html",
    "title": "Load Census tables via Tidycensus",
    "section": "",
    "text": "This script provides a few examples of how to load ACS tables from https://data.census.gov using the Tidycensus package.\nlibrary(tidycensus)\nlibrary(tidyverse)\nNote: A census API key is required to use tidycensus. Register for a Census API key at https://api.census.gov/data/key_signup.html. Once you’ve obtained a key, you can copy it to your .Renviron file by using the census_api_key() function.\n#sets Census API key (optional but encouraged)\ncensus_api_key(\"YOUR CENSUS API KEY HERE\", install = TRUE)\n\n# Reload your environment so you can use the key without restarting R.\nreadRenviron(\"~/.Renviron\")\n\n# You can check it with:\nSys.getenv(\"CENSUS_API_KEY\")",
    "crumbs": [
      "R code",
      "Load Census tables via Tidycensus"
    ]
  },
  {
    "objectID": "tidycensus_example.html#explore-available-variables-in-the-acs-5-year-sample",
    "href": "tidycensus_example.html#explore-available-variables-in-the-acs-5-year-sample",
    "title": "Load Census tables via Tidycensus",
    "section": "Explore available variables in the ACS 5-year sample",
    "text": "Explore available variables in the ACS 5-year sample\nUse the load_variables() function to load available tables derived from the 2021 ACS 5-year survey.\n\nSee a full list of available Census tables at https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.html\n\n\n# Show all available tables\n\nacs_2021_variables &lt;- load_variables(2021, \"acs5\", cache = TRUE)\n\nacs_2021_variables\n\n# A tibble: 27,886 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_001 Estimate!!Total:                         SEX BY AGE (W… tract    \n 2 B01001A_002 Estimate!!Total:!!Male:                  SEX BY AGE (W… tract    \n 3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   SEX BY AGE (W… tract    \n 4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years    SEX BY AGE (W… tract    \n 5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years  SEX BY AGE (W… tract    \n 6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years  SEX BY AGE (W… tract    \n 7 B01001A_007 Estimate!!Total:!!Male:!!18 and 19 years SEX BY AGE (W… tract    \n 8 B01001A_008 Estimate!!Total:!!Male:!!20 to 24 years  SEX BY AGE (W… tract    \n 9 B01001A_009 Estimate!!Total:!!Male:!!25 to 29 years  SEX BY AGE (W… tract    \n10 B01001A_010 Estimate!!Total:!!Male:!!30 to 34 years  SEX BY AGE (W… tract    \n# ℹ 27,876 more rows",
    "crumbs": [
      "R code",
      "Load Census tables via Tidycensus"
    ]
  },
  {
    "objectID": "tidycensus_example.html#examples",
    "href": "tidycensus_example.html#examples",
    "title": "Load Census tables via Tidycensus",
    "section": "Examples",
    "text": "Examples\n\nDemographic data\nThis example loads demographic data from the 2017-2021 ACS for counties in Michigan.\n\n# This option will retrieve geographic data from the Census\noptions(tigris_use_cache = TRUE)\n\n# Table B01001: Sex by Age\nMI_demographics &lt;- get_acs(table = \"B01001\",\n                           geography = \"county\",\n                           year = 2021,\n                           state = \"MI\",\n                           survey = \"acs5\")\n\nGetting data from the 2017-2021 5-year ACS\n\nMI_demographics\n\n# A tibble: 4,067 × 5\n   GEOID NAME                    variable   estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 26001 Alcona County, Michigan B01001_001    10138    NA\n 2 26001 Alcona County, Michigan B01001_002     5127    38\n 3 26001 Alcona County, Michigan B01001_003      169    12\n 4 26001 Alcona County, Michigan B01001_004      176    40\n 5 26001 Alcona County, Michigan B01001_005      190    36\n 6 26001 Alcona County, Michigan B01001_006      132    12\n 7 26001 Alcona County, Michigan B01001_007       68    17\n 8 26001 Alcona County, Michigan B01001_008       28    20\n 9 26001 Alcona County, Michigan B01001_009       47    18\n10 26001 Alcona County, Michigan B01001_010      110    24\n# ℹ 4,057 more rows\n\n\n\n\nMap of median household income in MI, by county\nThis example creates a map of median household income in Michigan from the 2017-2021 ACS, by county.\n\n# Plot household income by county\nMI_income &lt;- get_acs(\n  geography = \"county\", \n  state = \"MI\",\n  variables = \"B19013_001\",\n  year = 2021,\n  geometry = TRUE,\n)\n\nplot(MI_income[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nMap of median household income for Wayne County, MI, by tract\n\ndetroit_income &lt;- get_acs(\n  geography = \"tract\", \n  state = \"MI\",\n  county = \"Wayne\",\n  variables = \"B19013_001\",\n  year = 2021,\n  geometry = TRUE,\n)\n\nplot(detroit_income[\"estimate\"])\n\n\n\n\n\n\n\n\n\n\nTotal income below poverty level in Rhode Island by gender, 2009–2021\nThis example defines function to load multiple years and multiple demographic groups for table b17001\n\n# Poverty in Rhode Island: https://data.census.gov/table?q=B17001B\n\n#  Function to load multiple years of acs data\nload_acs_tables &lt;- function(x){\n  get_acs(geography = \"state\", \n          variables = c(total_count = \"B17001_001\",  \n                        count_income_below_poverty = \"B17001_002\",  \n                        count_income_below_poverty_level_male = \"B17001_003\",  \n                        count_income_below_poverty_level_female = \"B17001_017\"), \n          state = \"RI\", \n          year = x, \n          output = \"wide\") %&gt;% \n    #create year variable\n    mutate(year = x)\n}\n\n#load 2009:2018 5yr datasets with map_dfr()\nRI_Poverty_B &lt;- map_dfr(2009:2021, load_acs_tables)\n\nRI_Poverty_B\n\n# A tibble: 13 × 11\n   GEOID NAME         total_countE total_countM count_income_below_povertyE\n   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;                       &lt;dbl&gt;\n 1 44    Rhode Island      1019380          497                      118618\n 2 44    Rhode Island      1014029          440                      123396\n 3 44    Rhode Island      1012044          459                      129454\n 4 44    Rhode Island      1011137          430                      133462\n 5 44    Rhode Island      1010872          463                      137244\n 6 44    Rhode Island      1012806          364                      143996\n 7 44    Rhode Island      1013455          398                      144223\n 8 44    Rhode Island      1013916          310                      140161\n 9 44    Rhode Island      1015923          402                      136126\n10 44    Rhode Island      1016029          462                      133055\n11 44    Rhode Island      1016506          463                      125826\n12 44    Rhode Island      1017028          481                      117785\n13 44    Rhode Island      1050314          707                      118257\n# ℹ 6 more variables: count_income_below_povertyM &lt;dbl&gt;,\n#   count_income_below_poverty_level_maleE &lt;dbl&gt;,\n#   count_income_below_poverty_level_maleM &lt;dbl&gt;,\n#   count_income_below_poverty_level_femaleE &lt;dbl&gt;,\n#   count_income_below_poverty_level_femaleM &lt;dbl&gt;, year &lt;int&gt;\n\n\nA more complex function to load multiple demographic groups\n\nload_acs_tables2 &lt;- function(x,y){\n  get_acs(geography = \"state\", \n          variables = c(total_count = paste0(\"B17001\",y,\"_001\"),  \n                        count_income_below_poverty = paste0(\"B17001\",y,\"_002\"),  \n                        count_income_below_poverty_level_male = paste0(\"B17001\",y,\"_003\"),  \n                        count_income_below_poverty_level_female = paste0(\"B17001\",y,\"_017\")), \n          state = \"RI\", \n          year = x, \n          output = \"wide\") %&gt;% \n    #create variables to identify years and demographic groups\n    mutate(year = x,\n           group = y)\n}\n\n#create list of arguments to pass to function\ncrossargs &lt;- expand.grid(x=2009:2021, y=LETTERS[1:9])\n\n#load all data 2009 to 2021\nRI_Poverty &lt;- map2_dfr(crossargs$x, crossargs$y, load_acs_tables2)\n\nRI_Poverty\n\n# A tibble: 117 × 12\n   GEOID NAME         total_countE total_countM count_income_below_povertyE\n   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;                       &lt;dbl&gt;\n 1 44    Rhode Island       844002         2476                       74676\n 2 44    Rhode Island       832540         2445                       76711\n 3 44    Rhode Island       831102         2675                       80691\n 4 44    Rhode Island       827707         2583                       84578\n 5 44    Rhode Island       824245         3051                       86232\n 6 44    Rhode Island       825456         2895                       92849\n 7 44    Rhode Island       823532         2843                       93988\n 8 44    Rhode Island       822275         3089                       92245\n 9 44    Rhode Island       823518         3001                       89596\n10 44    Rhode Island       823420         2950                       89282\n# ℹ 107 more rows\n# ℹ 7 more variables: count_income_below_povertyM &lt;dbl&gt;,\n#   count_income_below_poverty_level_maleE &lt;dbl&gt;,\n#   count_income_below_poverty_level_maleM &lt;dbl&gt;,\n#   count_income_below_poverty_level_femaleE &lt;dbl&gt;,\n#   count_income_below_poverty_level_femaleM &lt;dbl&gt;, year &lt;int&gt;, group &lt;fct&gt;\n\n\nSee more examples from Tidycensus at https://walker-data.com/tidycensus/articles/basic-usage.html",
    "crumbs": [
      "R code",
      "Load Census tables via Tidycensus"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Other Resources",
    "section": "",
    "text": "If you can’t find the answers you’re looking for in our tutorials or elsewhere on our site, check out these external resources. If you’re still stuck, feel free to reach out to ecohn@epi.org.\nR@Urban\n\nR@Urban is a great resource from the Urban Institute. This site covers analysis, visualization, mapping, and more.\n\nR for Data Science by Garrett Grolemund and Hadley Wickham\n\nR for Data Science is a popular book that covers many basic concepts in R, aimed specifically at beginners.\n\nDataScienceR Github\n\nThis Github repository contains a list of R tutorials and packages for Data Science, NLP and Machine Learning. It also serves as a reference guide for several common data analysis tasks (though most are minimally annotated).\n\nCausal Inference: The Mixtape\n\nThis ebook provides an introduction to causal inference using a range of modeling techniques and coding instructions for both R and Stata.\n\nCausal Inference in R\n\nThis book focuses explicitly on causal inference in R and is aimed at intermediate coders. The preface outlines what level of comfort with R the reader should have before using this book.\n\n\n\n\n Back to top",
    "crumbs": [
      "Learn R",
      "Other Resources"
    ]
  },
  {
    "objectID": "foraging_for_data.html",
    "href": "foraging_for_data.html",
    "title": "Foraging for Data in the Wild 2025",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on August 26th, 2025.\nMissed this talk? See the recording here: Foraging for Data in the Wild.\nPasscode: 4aging2025!",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#welcome-to-the-foraging-for-data-in-the-wild-2025-landing-page",
    "href": "foraging_for_data.html#welcome-to-the-foraging-for-data-in-the-wild-2025-landing-page",
    "title": "Foraging for Data in the Wild 2025",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on August 26th, 2025.\nMissed this talk? See the recording here: Foraging for Data in the Wild.\nPasscode: 4aging2025!",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#unemployment-insurance-claims-by-state",
    "href": "foraging_for_data.html#unemployment-insurance-claims-by-state",
    "title": "Foraging for Data in the Wild 2025",
    "section": "Unemployment Insurance Claims by State",
    "text": "Unemployment Insurance Claims by State\n\n1) Import libraries\n\nlibrary(tidyverse)\nlibrary(data.table) # setnames(), map DOL data dictionary to the raw data\nlibrary(lubridate) # data helper functions to recast messy data as date type\nlibrary(openxlsx2) # mapping data and setting formatting for excel wb\n\n\n\n2) Download raw data\nUI IC & CC (NSA) comes for ETA 539, which can be found on the DOL ETA website.\nUse the command-line utility function wget to trigger and direct the download of exportable online data. Wrap the statement in system() to direct execution to the terminal\nNOTE: Some users encounter a 403 error with these commands. If this happens, manually download the ar539.csv file and place it in your working directory.\n\n# \"wget -N\" omits download if data has not been updated \"-P\" sets the file destination\"\nsystem(\"wget -N https://oui.doleta.gov/unemploy/csv/ar539.csv  -P data/\")\n\n\n\n3) Wrangle data\nReplace raw variable names using a user-defined data dictionary (download it here).\nThe data dictionary is a combination of the DOL ETA 539 Data Map (found on Data Downloads page) and the ETA 401 handook “Item by Item Instructions.”\n\n# read in data dictionary\ndata_dictionary &lt;- read.csv(\"data/eta539_var_names.csv\")\n\n\n\n4) Cleanse and manipulate data\nUse data.table::setnames() to apply the data dictionary to the raw data. Recast date columns to be of Date class type (or data storage type). “Wild data” often stores dates as str (or string) class type, so use functions from the ?lubridate package to easily handle and manipulate date types.\n\n# Cleanse raw data\nraw_data &lt;- read.csv(\"data/ar539.csv\") |&gt;\n  # use $ operator to select column from data frame\n  setnames(old = data_dictionary$dol_code, new = data_dictionary$dol_title) |&gt;\n  # format date as class 'Date' \n  mutate(report_date = mdy(report_date),\n         reflect_week_ending = mdy(reflect_week_ending))\n\n\n\n5) Analyze\nUse dplyr::mutate() to create new columns. Calculate non-seasonally adjusted initial claims as state UI initial claims + short-term compensation (or workshare) initial claims and non-seasonally adjusted continued claims as state UI continued claims + short-tern compensation (or workshare) continued claims.\n\n# Initial claims (NSA) \ninitial_claims &lt;- raw_data  |&gt; \n  # Initial Claims & Continued Claims, non seasonally adjusted (as seen here: https://oui.doleta.gov/unemploy/claims.asp) \n  # UI IC is calculated from c3 (initial claims) & c7 (short time compensation workshare)\n  mutate(nsa_initial_claims = state_ui_initial_claims + stc_workshare_equivalent_initial_claims) |&gt; \n  select(state, report_date, nsa_initial_claims) |&gt; \n  # filter out unstable reporting\n  filter(report_date &gt;= '1987-01-01') |&gt;\n  # transform into wide format - each state is own column\n  #note: https://bookdown.org/Maxine/r4ds/pivoting.html\n  pivot_wider(id_cols = report_date, names_from = state, values_from = nsa_initial_claims) |&gt; \n  # remove Puerto Rico and US Virgin Islands\n  select(-PR, -VI) |&gt; \n  # replace state abbreviation with state name\n  setnames(old = state.abb, new = state.name) |&gt; \n  # rename DC (not included in state* utility functions)\n  rename(`District of Columbia` = DC) |&gt; \n  # sort data\n  arrange(report_date)\n\n# Continued claims (NSA) \ncontinued_claims &lt;- raw_data |&gt; \n  # Initial Claims & Continued Claims, non seasonally adjusted (as seen here: https://oui.doleta.gov/unemploy/claims.asp) \n  # UI CC is calculated from c8 & c12\n  mutate(nsa_continued_claims = state_ui_adjusted_continued_weeks_claimed + stc_workshare_equivalent_continued_weeks_claimed) |&gt; \n  select(state, reflect_week_ending, nsa_continued_claims) |&gt; \n  # filter out unstable reporting\n  filter(reflect_week_ending &gt;= '1987-01-01') |&gt;\n  # transform into wide format - each state is own column\n  pivot_wider(id_cols = reflect_week_ending, names_from = state, values_from = nsa_continued_claims) |&gt; \n  # remove Puerto Rico & US Virgin Islands\n  select(-PR, -VI) |&gt; \n  # replace state abbreviation with state name\n  setnames(old = state.abb, new = state.name) |&gt; \n  # replace DC (not included in state.* utility data)\n  rename(`District of Columbia` = DC) |&gt; \n  # sort data \n  arrange(reflect_week_ending)\n\n\n\n6) Export data\nUse ?openxlsx2 to create excel workbooks, which support multiple tabs and backend formatting. This is a great way to generate replicable final products. Note that openxlsx2 uses the $ pipe operator to modify workbook objects created by openxlsx2::wb_workbook(). Create worksheets, add data, and use functions such as opnexlsx2::wb_set_col_widths() and openxlsx2::add_cell_style() to stylize the workbook.\n\n# create WB object\nwb &lt;- wb_workbook()\n\n# write UI state IC to WB object\n#note: $ - pipe operator in openxlsx2 \nwb$\n  # add new worksheet\n  add_worksheet(sheet = \"Initial claims\")$\n  # add data to worksheet\n  add_data(x = initial_claims)$\n  # set columm widths\n  set_col_widths(cols = 2:ncol(initial_claims), widths = 15)$\n  # format column headers\n  add_cell_style(dims = wb_dims(rows = 1, cols = 2:ncol(initial_claims)), \n                 wrap_text = TRUE, horizontal = \"center\", vertical = \"center\")$\n  # repeat for continued claims\n  add_worksheet(sheet = \"Continued claims\")$\n  add_data(x = continued_claims)$\n  set_col_widths(cols = 2:ncol(continued_claims), widths = 15)$\n  add_cell_style(dims = wb_dims(rows = 1, cols = 2:ncol(continued_claims)), \n                 wrap_text = TRUE, horizontal = \"center\", vertical = \"center\")$\n  # save workbook to output folder\n  save(\"output/state_ui.xlsx\")",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#using-the-qcew-to-measure-employment-growth-in-data-centers-by-state",
    "href": "foraging_for_data.html#using-the-qcew-to-measure-employment-growth-in-data-centers-by-state",
    "title": "Foraging for Data in the Wild 2025",
    "section": "Using the QCEW to measure employment growth in data centers by state",
    "text": "Using the QCEW to measure employment growth in data centers by state\n\nObjectives\n\nCreate data by iteratively calling a function.\nBind/append data frames to create a large dataset.\nRead data from a .CSV directly from the web into R.\nHarmonize data types.\nUse joins to combine datasets.\nFilter using string detection.\nReorder variables using select(), arrange(), and/or relocate() functions.\nUse some tricks to create quarterly and monthly data types with lubridate.\nMeasure employment changes for NAICS industry 518.\nBonus: quick visualization with ggplot2!\n\n\nQuestion. How has “data center” employment (NAICS 518) grown since late 2022, and in which states has it grown the most?\nTo answer these questions, we’ll fetch 2022–2024 quarterly data for NAICS 518: Computing infrastructure providers, data processing, web hosting, and related services.\n** Disclaimer**: I’m not sure if this is the most appropriate NAICS code, but it makes for a good exercise! Depending on your research question, you may want to refine how you select industry codes (e.g., include selected sub‑industries or complementary sectors.).",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#load-libraries",
    "href": "foraging_for_data.html#load-libraries",
    "title": "Foraging for Data in the Wild 2025",
    "section": "1) Load libraries",
    "text": "1) Load libraries\n\nlibrary(tidyverse)\nlibrary(lubridate)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#bls-functions-for-loading-qcew-data",
    "href": "foraging_for_data.html#bls-functions-for-loading-qcew-data",
    "title": "Foraging for Data in the Wild 2025",
    "section": "2) BLS functions for loading QCEW data",
    "text": "2) BLS functions for loading QCEW data\nThe BLS conveniently provides a script and three functions for R users to load QCEW data directly into R! Below are two helpers adapted for this module. They construct an API URL and return a data frame for the requested year/quarter/industry or area.\nThese resources can be downloaded from this page: https://www.bls.gov/cew/additional-resources/open-data/sample-code.htm#RSCRIPT\n\n# This function loads all industries for one geographical area\nqcewGetAreaData &lt;- function(year, qtr, area) {\n  url &lt;- \"http://data.bls.gov/cew/data/api/YEAR/QTR/area/AREA.csv\"\n  url &lt;- sub(\"YEAR\", year, url, ignore.case=FALSE)\n  url &lt;- sub(\"QTR\", tolower(qtr), url, ignore.case=FALSE)\n  url &lt;- sub(\"AREA\", toupper(area), url, ignore.case=FALSE)\n  read.csv(url, header = TRUE, sep = \",\", quote=\"\\\"\", dec=\".\", na.strings=\" \", skip=0)\n}\n\n# This function loads one industry for all geographical areas\nqcewGetIndustryData &lt;- function (year, qtr, industry) {\n    url &lt;- \"http://data.bls.gov/cew/data/api/YEAR/QTR/industry/INDUSTRY.csv\"\n    url &lt;- sub(\"YEAR\", year, url, ignore.case=FALSE)\n    url &lt;- sub(\"QTR\", tolower(qtr), url, ignore.case=FALSE)\n    url &lt;- sub(\"INDUSTRY\", industry, url, ignore.case=FALSE)\n    read.csv(url, header = TRUE, sep = \",\", quote=\"\\\"\", dec=\".\", na.strings=\" \", skip=0)\n}\n\n# Quick examples (not evaluated by default)\n# In ex. 1, we call the qcewGetAreaData() function, passing parameters for year, quarter, and areafips/geography i.e. year = 2015, quarter = 1, areafips = 26000 or Michigan.\n# We then assign the data called to a variable called MichiganData!\n\n#  MichiganData &lt;- qcewGetAreaData(\"2015\", \"1\", \"26000\")\n#  Construction &lt;- qcewGetIndustryData(\"2015\", \"1\", \"1012\")",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#qcew-data-pull",
    "href": "foraging_for_data.html#qcew-data-pull",
    "title": "Foraging for Data in the Wild 2025",
    "section": "3) QCEW data pull",
    "text": "3) QCEW data pull\nSince the example functions load only one quarter at a time, we’ll want to make some modifications. Instead of calling the function 12 times by hand, we’ll build a grid of parameters and map across it.\n\n# Set our parameters \nyears &lt;- 2022:2024\nquarters &lt;- 1:4\nindustries &lt;- c('518')   \n\n# create 12 combinations (3 years × 4 quarters × 1 industry) to pass through pmap()\ncombos &lt;- tidyr::crossing(year = years, qtr = quarters, industry = industries)\n\n# Takes the combinations, runs qcewGetIndustryData() once for each\n# returns a list of \"small\" dataframes, which we combine into a large one called QCEW raw\n\nqcew_raw &lt;- pmap(combos, function(year, qtr, industry){\n  qcewGetIndustryData(year, qtr, industry)\n  }) |&gt; \n  # Combines all dataframes by appending/binding \"rows\" \n  bind_rows()\n\n# Explores our data\nglimpse(qcew_raw)\n\nRows: 29,108\nColumns: 42\n$ area_fips                       &lt;chr&gt; \"01000\", \"01000\", \"01001\", \"01003\", \"0…\n$ own_code                        &lt;int&gt; 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ industry_code                   &lt;int&gt; 518, 518, 518, 518, 518, 518, 518, 518…\n$ agglvl_code                     &lt;int&gt; 55, 55, 75, 75, 75, 75, 75, 75, 75, 75…\n$ size_code                       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ year                            &lt;int&gt; 2022, 2022, 2022, 2022, 2022, 2022, 20…\n$ qtr                             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ disclosure_code                 &lt;chr&gt; \"\", \"\", \"N\", \"\", \"N\", \"\", \"N\", \"N\", \"N…\n$ qtrly_estabs                    &lt;int&gt; 1, 1019, 2, 31, 1, 4, 2, 1, 1, 0, 1, 6…\n$ month1_emplvl                   &lt;int&gt; 13, 2421, 0, 72, 0, 7, 0, 0, 0, 0, 0, …\n$ month2_emplvl                   &lt;int&gt; 13, 2511, 0, 73, 0, 7, 0, 0, 0, 0, 0, …\n$ month3_emplvl                   &lt;int&gt; 13, 2523, 0, 79, 0, 7, 0, 0, 0, 0, 0, …\n$ total_qtrly_wages               &lt;dbl&gt; 249734, 60137580, 0, 1271327, 0, 67735…\n$ taxable_qtrly_wages             &lt;dbl&gt; 0, 20180327, 0, 574492, 0, 47119, 0, 0…\n$ qtrly_contributions             &lt;int&gt; 0, 322219, 0, 8174, 0, 2132, 0, 0, 0, …\n$ avg_wkly_wage                   &lt;int&gt; 1478, 1862, 0, 1310, 0, 744, 0, 0, 0, …\n$ lq_disclosure_code              &lt;chr&gt; \"\", \"\", \"N\", \"\", \"N\", \"\", \"N\", \"N\", \"N…\n$ lq_qtrly_estabs                 &lt;dbl&gt; 4.05, 1.81, 0.51, 1.06, 0.29, 0.39, 0.…\n$ lq_month1_emplvl                &lt;dbl&gt; 1.07, 0.40, 0.00, 0.31, 0.00, 0.05, 0.…\n$ lq_month2_emplvl                &lt;dbl&gt; 1.06, 0.42, 0.00, 0.31, 0.00, 0.05, 0.…\n$ lq_month3_emplvl                &lt;dbl&gt; 1.05, 0.42, 0.00, 0.33, 0.00, 0.05, 0.…\n$ lq_total_qtrly_wages            &lt;dbl&gt; 1.29, 0.30, 0.00, 0.21, 0.00, 0.02, 0.…\n$ lq_taxable_qtrly_wages          &lt;dbl&gt; 0.00, 0.33, 0.00, 0.25, 0.00, 0.04, 0.…\n$ lq_qtrly_contributions          &lt;dbl&gt; 0.00, 0.40, 0.00, 0.27, 0.00, 0.15, 0.…\n$ lq_avg_wkly_wage                &lt;dbl&gt; 1.21, 0.73, 0.00, 0.65, 0.00, 0.36, 0.…\n$ oty_disclosure_code             &lt;chr&gt; \"N\", \"\", \"N\", \"\", \"N\", \"\", \"N\", \"N\", \"…\n$ oty_qtrly_estabs_chg            &lt;int&gt; 0, 318, 0, 10, 1, 0, 1, -1, 1, 0, 0, 1…\n$ oty_qtrly_estabs_pct_chg        &lt;dbl&gt; 0.0, 45.4, 0.0, 47.6, 100.0, 0.0, 100.…\n$ oty_month1_emplvl_chg           &lt;int&gt; 0, 149, 0, 0, 0, 3, 0, 0, 0, 0, 0, -1,…\n$ oty_month1_emplvl_pct_chg       &lt;dbl&gt; 0.0, 6.6, 0.0, 0.0, 0.0, 75.0, 0.0, 0.…\n$ oty_month2_emplvl_chg           &lt;int&gt; 0, 216, 0, 0, 0, 2, 0, 0, 0, 0, 0, -1,…\n$ oty_month2_emplvl_pct_chg       &lt;dbl&gt; 0.0, 9.4, 0.0, 0.0, 0.0, 40.0, 0.0, 0.…\n$ oty_month3_emplvl_chg           &lt;int&gt; 0, 201, 0, 3, 0, 2, 0, 0, 0, 0, 0, -1,…\n$ oty_month3_emplvl_pct_chg       &lt;dbl&gt; 0.0, 8.7, 0.0, 3.9, 0.0, 40.0, 0.0, 0.…\n$ oty_total_qtrly_wages_chg       &lt;dbl&gt; 0, 13354348, 0, 210886, 0, 5327, 0, 0,…\n$ oty_total_qtrly_wages_pct_chg   &lt;dbl&gt; 0.0, 28.5, 0.0, 19.9, 0.0, 8.5, 0.0, 0…\n$ oty_taxable_qtrly_wages_chg     &lt;int&gt; 0, 2694560, 0, 39716, 0, 12909, 0, 0, …\n$ oty_taxable_qtrly_wages_pct_chg &lt;dbl&gt; 0.0, 15.4, 0.0, 7.4, 0.0, 37.7, 0.0, 0…\n$ oty_qtrly_contributions_chg     &lt;int&gt; 0, -46111, 0, -4990, 0, 371, 0, 0, 0, …\n$ oty_qtrly_contributions_pct_chg &lt;dbl&gt; 0.0, -12.5, 0.0, -37.9, 0.0, 21.1, 0.0…\n$ oty_avg_wkly_wage_chg           &lt;int&gt; 0, 295, 0, 203, 0, -285, 0, 0, 0, 0, 0…\n$ oty_avg_wkly_wage_pct_chg       &lt;dbl&gt; 0.0, 18.8, 0.0, 18.3, 0.0, -27.7, 0.0,…",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#add-readable-labels-industry-area-titles",
    "href": "foraging_for_data.html#add-readable-labels-industry-area-titles",
    "title": "Foraging for Data in the Wild 2025",
    "section": "4) Add readable labels (industry & area titles)",
    "text": "4) Add readable labels (industry & area titles)\nOur dataframe is loaded! But it’s not very legible. For starters, some geographic and industry titles would help.\nThe BLS provides a codebook for parsing our data https://www.bls.gov/cew/about-data/downloadable-file-layouts/quarterly/naics-based-quarterly-layout.htm. We’ll load these directly into R.\n** disclaimer ** depending on your IT’s security settings, you may not be able to directly download these links into R. If you encounter this issue (like I did), you can navigate directly to the .htm links below, download the .CSV files, and place them in your working directory.\nIndustry titles downloaded from https://www.bls.gov/cew/classifications/industry/industry-titles.htm\nArea titles downloaded from https://www.bls.gov/cew/classifications/areas/qcew-area-titles.htm\n\n# link to csv files on the BLS QCEW site\nind_title_url &lt;- 'https://www.bls.gov/cew/classifications/industry/industry-titles.csv'\narea_title_url &lt;- 'https://www.bls.gov/cew/classifications/areas/area-titles-csv.csv'\n\n# # Read csv files directly into R from the QCEW page\n# ind_titles  &lt;- read_csv(ind_title_url)\n# area_titles &lt;- read_csv(area_title_url)\n\nind_titles &lt;- read_csv('data/industry_titles.csv')\narea_titles &lt;- read_csv('data/area-titles-csv.csv')\n\n\n4a) First attempt at joining labels\nWe’ll first attempt a natural left_join() on industry_code. A left join keeps all rows from our main dataset (qcew, the “x” table) and adds matches from ind_titles (the “y” table). By default, it matches on any identically named columns (a “natural join”), but we could also set the key explicitly using the by = argument.\nSee Section 19.4 How do joins work? from R for Data Science (2e) for some great visualizations.\n\n# Can you spot the difference?\nglimpse(qcew_raw$industry_code)   # likely &lt;int&gt; / &lt;dbl&gt;\n\n int [1:29108] 518 518 518 518 518 518 518 518 518 518 ...\n\nglimpse(ind_titles$industry_code) # likely &lt;chr&gt;\n\n chr [1:2678] \"10\" \"101\" \"1011\" \"1012\" \"1013\" \"102\" \"1021\" \"1022\" \"1023\" ...\n\n# What happens?\nqcew_raw |&gt;\n  dplyr::left_join(ind_titles)\n\nJoining with `by = join_by(industry_code)`\n\n\nError in `dplyr::left_join()`:\n! Can't join `x$industry_code` with `y$industry_code` due to\n  incompatible types.\nℹ `x$industry_code` is a &lt;integer&gt;.\nℹ `y$industry_code` is a &lt;character&gt;.\n\n\n\n\n4b) Harmonizing data types to join\nMaldito! We have an issue. Despite matching variable names, industry_code in datasets “x” and “y” are different datatypes. in “x”, it’s an integer, a numeric type. in “y” it’s a character. In order to merge we need our datatypes to be the same.\nFortunately, is an easy fix. We can convert one of two columns to match data types. Which should we convert? Well, it’s easier to go from character to numeric here. A quirk of R is that numerics don’t have leading zeros, but strings can. You wouldn’t write 100 as 0100, or 00100, right? Right??\nSince NAICS 3‑digit codes like 518 don’t have leading zeros, converting ind_titles$industry_code to numeric is safe here. (General tip: if codes are required to have leading zeros, keep them as character in both tables.)\n\n# Make the join keys the same type\nind_titles &lt;- ind_titles |&gt;\n  mutate(industry_code = as.numeric(industry_code))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `industry_code = as.numeric(industry_code)`.\nCaused by warning:\n! NAs introduced by coercion\n\n# Now the join works:\nqcew_ind_clean &lt;- qcew_raw |&gt;\n  # natural join on identical names\n  left_join(ind_titles)  \n\nJoining with `by = join_by(industry_code)`\n\n# We could also be explicit about which variable to join by\n# qcew_ind_clean &lt;- qcew_raw |&gt; \n#   left_join(ind_titles, by = 'industry_code')\n\n# If our variable names differ, we could map them using \n# left_join(ind_titles, by = c(\"industry_code\" = \"ind_code\"))\n\n\n\n4c) Cleaning continued\nNow join titles and keep only variables we need. We also want limit to private‑sector, statewide data only. We know from the QCEW codebook that: own_code == 5 and agglvl_code == 55.\nSee this codebook for more detail: QCEW Field Layouts for NAICS-Based, Quarterly CSV Files\n\n# (If needed) ensure area_fips types match before joining area_titles\n# area_titles &lt;- area_titles |&gt; mutate(area_fips = as.character(area_fips))\n\nqcew_clean &lt;- qcew_ind_clean |&gt;\n  left_join(area_titles) |&gt; \n  # Too many variables we don't need, let's restrict and re-order using select\n  select(\n    year, qtr, area_fips, area_title, industry_code,\n    industry_title, own_code, agglvl_code, month1_emplvl, month2_emplvl,\n    month3_emplvl) |&gt; \n\n  # Now, why are seeing two rows for each state? Again, per the codebook, QCEW contains data for public and private sector industries\n  # We can filter for private sector data using own_code == 5\n  filter(own_code == 5) |&gt; \n  filter(agglvl_code == 55)\n\nJoining with `by = join_by(area_fips)`\n\n# Tip: An alternative way to filter for statewide data. If you don’t have a variable like agglvl_code, you can use string detection to filter statewide rows. We prefer to use code‑book filters when available.\n# filter(str_detect(area_title, \" -- Statewide\"))",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#quarterly-dates-averages",
    "href": "foraging_for_data.html#quarterly-dates-averages",
    "title": "Foraging for Data in the Wild 2025",
    "section": "5) Quarterly dates & averages",
    "text": "5) Quarterly dates & averages\nCreate a quarterly average employment measure and a proper quarterly date using lubridate’s yq().\n\nqcew_qtr &lt;- qcew_clean |&gt;\n  mutate(\n    qtr_avg_emp = (month1_emplvl + month2_emplvl + month3_emplvl) / 3,\n    qdate       = yq(paste(year, qtr, sep = \" Q\"))\n  )",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#reshape-to-create-statebycolumn-tables",
    "href": "foraging_for_data.html#reshape-to-create-statebycolumn-tables",
    "title": "Foraging for Data in the Wild 2025",
    "section": "6) Reshape to create state‑by‑column tables",
    "text": "6) Reshape to create state‑by‑column tables\nNow that we have quarterly data, we’ll use pivot_wider() to create a table of quarterly data that is long by date.\n\nstate_qtr_table &lt;- qcew_qtr |&gt;\n  mutate(state = str_replace(area_title, \" -- Statewide\", \"\")) |&gt;\n  select(qdate, state, qtr_avg_emp) |&gt;\n  pivot_wider(id_cols = qdate, names_from = state, values_from = qtr_avg_emp)\n\nstate_qtr_table\n\n# A tibble: 12 × 54\n   qdate      Alabama Alaska Arizona Arkansas California Colorado Connecticut\n   &lt;date&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 2022-01-01   2485    96.3  12099.    4383.     66262.   16167.       3649.\n 2 2022-04-01   2772.  108    12538.    4546.     71640.   16592        3759.\n 3 2022-07-01   3060.  112.   12322     4705.     72875    16611.       3883.\n 4 2022-10-01   3143    97.3  12230     4746.     74211    16504        3795.\n 5 2023-01-01   3563.  117.   12441     4786.     84623    16157        3795 \n 6 2023-04-01   3699.  132.   12235.    4589      83776.   15891.       3778 \n 7 2023-07-01   3872   131.   11652.    4521.     83244.   15563        3750.\n 8 2023-10-01   3875   121    11905.    4494.     81299    15325.       3755.\n 9 2024-01-01   3738   108.   11401     4328      79753.   15234.       4081.\n10 2024-04-01   3812.  110.   11726.    4184.     79390.   15184        4314.\n11 2024-07-01   3929   130.   11374.    4170.     79600    15082.       4371 \n12 2024-10-01   3941   119.   11179.    4136.     78681.   14996.       4311.\n# ℹ 46 more variables: Delaware &lt;dbl&gt;,\n#   `District of Columbia, not unknown` &lt;dbl&gt;, Florida &lt;dbl&gt;, Georgia &lt;dbl&gt;,\n#   Hawaii &lt;dbl&gt;, Idaho &lt;dbl&gt;, Illinois &lt;dbl&gt;, Indiana &lt;dbl&gt;, Iowa &lt;dbl&gt;,\n#   Kansas &lt;dbl&gt;, Kentucky &lt;dbl&gt;, Louisiana &lt;dbl&gt;, Maine &lt;dbl&gt;, Maryland &lt;dbl&gt;,\n#   Massachusetts &lt;dbl&gt;, Michigan &lt;dbl&gt;, Minnesota &lt;dbl&gt;, Mississippi &lt;dbl&gt;,\n#   Missouri &lt;dbl&gt;, Montana &lt;dbl&gt;, Nebraska &lt;dbl&gt;, Nevada &lt;dbl&gt;,\n#   `New Hampshire` &lt;dbl&gt;, `New Jersey` &lt;dbl&gt;, `New Mexico` &lt;dbl&gt;, …",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#monthly-data-from-qcew",
    "href": "foraging_for_data.html#monthly-data-from-qcew",
    "title": "Foraging for Data in the Wild 2025",
    "section": "7) Monthly data from QCEW",
    "text": "7) Monthly data from QCEW\nThe QCEW provides monthly employment levels in each quarter! With a few tweaks to our data frame, we can produce a monthly series, giving us a more granular look at how data center employment has grown since 2022. Each QCEW quarter reports employment for its three months. We can unpivot those columns to build a monthly time series.\n\nqcew_monthly &lt;- qcew_clean |&gt;\n  pivot_longer(\n    cols = starts_with(\"month\"),\n    names_to = \"month_in_qtr\",\n    names_pattern = \"month(\\\\d+)_emplvl\",\n    values_to = \"emplvl\"\n  ) |&gt;\n  mutate(\n    month_in_qtr = as.integer(month_in_qtr),\n    month        = (qtr - 1) * 3 + month_in_qtr,\n    date         = make_date(year, month, 1),\n  ) |&gt;\n  select(area_title, area_fips, industry_code, year, qtr, date, emplvl) |&gt;\n  arrange(area_title, year, qtr, date) |&gt; \n  # let's clean up our state names!\n  mutate(state = str_replace(area_title, \" -- Statewide\", \"\"))\n\nhead(qcew_monthly)\n\n# A tibble: 6 × 8\n  area_title         area_fips industry_code  year   qtr date       emplvl state\n  &lt;chr&gt;              &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt;      &lt;int&gt; &lt;chr&gt;\n1 Alabama -- Statew… 01000               518  2022     1 2022-01-01   2421 Alab…\n2 Alabama -- Statew… 01000               518  2022     1 2022-02-01   2511 Alab…\n3 Alabama -- Statew… 01000               518  2022     1 2022-03-01   2523 Alab…\n4 Alabama -- Statew… 01000               518  2022     2 2022-04-01   2722 Alab…\n5 Alabama -- Statew… 01000               518  2022     2 2022-05-01   2795 Alab…\n6 Alabama -- Statew… 01000               518  2022     2 2022-06-01   2798 Alab…",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#measure-employment-growth-since-late-2022",
    "href": "foraging_for_data.html#measure-employment-growth-since-late-2022",
    "title": "Foraging for Data in the Wild 2025",
    "section": "8) Measure employment growth since late 2022",
    "text": "8) Measure employment growth since late 2022\nFor a simple comparison, compute percentage change from November 2022 to the latest available month for each state. (If a state is missing November specifically, we’ll use the first available month on or after 2022‑11‑01.)\n\nstate_growth &lt;- qcew_monthly |&gt;\n  summarize(\n    start_emplvl = first(emplvl),\n    end_emplvl   = last(emplvl),\n    start_date   = first(date),\n    end_date     = last(date),\n    .by=state) |&gt;\n  mutate(\n    emp_change = (end_emplvl - start_emplvl),\n    pct_change = (end_emplvl / start_emplvl - 1) * 100) |&gt;\n  # arrange data in descending order by pct_change\n  arrange(desc(pct_change))\n\nstate_growth |&gt; slice_head(n = 10)\n\n# A tibble: 10 × 7\n   state     start_emplvl end_emplvl start_date end_date   emp_change pct_change\n   &lt;chr&gt;            &lt;int&gt;      &lt;int&gt; &lt;date&gt;     &lt;date&gt;          &lt;int&gt;      &lt;dbl&gt;\n 1 Alabama           2421       3929 2022-01-01 2024-12-01       1508       62.3\n 2 Idaho              903       1296 2022-01-01 2024-12-01        393       43.5\n 3 South Da…          293        405 2022-01-01 2024-12-01        112       38.2\n 4 Alaska              89        120 2022-01-01 2024-12-01         31       34.8\n 5 Wyoming            163        217 2022-01-01 2024-12-01         54       33.1\n 6 New Jers…        12474      16072 2022-01-01 2024-12-01       3598       28.8\n 7 Maryland          4377       5631 2022-01-01 2024-12-01       1254       28.6\n 8 Rhode Is…          625        787 2022-01-01 2024-12-01        162       25.9\n 9 New Hamp…         1465       1822 2022-01-01 2024-12-01        357       24.4\n10 West Vir…         1182       1439 2022-01-01 2024-12-01        257       21.7",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#bonus-making-a-quick-visualization",
    "href": "foraging_for_data.html#bonus-making-a-quick-visualization",
    "title": "Foraging for Data in the Wild 2025",
    "section": "9) Bonus: Making a quick visualization",
    "text": "9) Bonus: Making a quick visualization\nLets plot a few states using ggplot2\nP.S. check out viz_workshop.qmd for some more ggplot examples!\n\nsel_state_data &lt;- qcew_monthly |&gt; \n  filter(state %in% c('Texas', 'Arkansas', 'Louisiana'))\n\nggplot(data = sel_state_data, aes(x=date, y=emplvl, color=state)) +\n  geom_line() +\n  labs(\n    title = \"Monthly employment (NAICS 518)\",\n    x = NULL, y = \"Employment level\",\n    color = \"State\"\n  )",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "foraging_for_data.html#notes-on-reproducibility",
    "href": "foraging_for_data.html#notes-on-reproducibility",
    "title": "Foraging for Data in the Wild 2025",
    "section": "10) Notes on reproducibility",
    "text": "10) Notes on reproducibility\n\nPro-tips: - This module reads files directly from bls.gov; those URLs occasionally change. If a link breaks, visit the QCEW classifications pages to refresh the URLs. - Consult QCEW layout/codebook to confirm variable meanings & aggregation levels for your projects.\n\nHappy coding!🕺",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Foraging for Data in the Wild 2025"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "Installing R & RStudio EARN Talk",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on July 29th, 2025.\nMissed this talk? See the recording here: Preparing for Analysis with R: A guided tutorial for installing R and RStudio.\nPasscode: Prep4R2025!\nPresentation slides",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#welcome-to-the-preparing-for-analysis-with-r-landing-page",
    "href": "intro_to_r.html#welcome-to-the-preparing-for-analysis-with-r-landing-page",
    "title": "Installing R & RStudio EARN Talk",
    "section": "",
    "text": "This training and code workflow was originally delivered as an EARN Talk on July 29th, 2025.\nMissed this talk? See the recording here: Preparing for Analysis with R: A guided tutorial for installing R and RStudio.\nPasscode: Prep4R2025!\nPresentation slides",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#workflow-example",
    "href": "intro_to_r.html#workflow-example",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Workflow example",
    "text": "Workflow example\nThis example demonstrates a simple data analysis workflow.\nIn short, we will use the Tidyverse package to load a dataset from a .csv file, describe and analyze the dataset, and export a final analysis to a new .csv file.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#load-a-library",
    "href": "intro_to_r.html#load-a-library",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Load a library",
    "text": "Load a library\nRecall, you can install R packages using the install.packages() command.\n\ninstall.packages('tidyverse')\n\nAfter installing a package, you can load it using the library() command.\n\n# Loading a library \nlibrary(tidyverse)",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#read-data-from-a-csv-file",
    "href": "intro_to_r.html#read-data-from-a-csv-file",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Read data from a csv file",
    "text": "Read data from a csv file\n\nDownload the dataset at this link: counties_per_capita_income.csv\nPlace the dataset in your root directory In this case, our data lives in a folder named “data” within this root directory. You can name your folder whatever you’d like!\nUse the read.csv() function to load the data into R.\n\n\ncounties_income &lt;- read.csv(\"data/counties_per_capita_income.csv\")",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#descriptive-analysis",
    "href": "intro_to_r.html#descriptive-analysis",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nSome useful commands to describe your dataset\n\n# Print the number of rows and columns in your table \ndim(counties_income)\n\n[1] 3231    8\n\n# View the top 5 rows of your dataset\nhead(counties_income)\n\n             county     states statefips   pci household_income family_income\n1   New York County   New York        36 76592            69659         86553\n2         Arlington   Virginia        51 62018           103208        139244\n3 Falls Church City   Virginia        51 59088           120000        152857\n4             Marin California         6 56791            90839        117357\n5       Santa Clara California         6 56248           124055        124055\n6   Alexandria City   Virginia        51 54608            85706        107511\n  population num_of_households\n1    1628706            759460\n2     214861             94454\n3      12731              5020\n4     254643            102912\n5    1927852            640215\n6     143684             65369\n\n# List the variable names of your dataset\nnames(counties_income)\n\n[1] \"county\"            \"states\"            \"statefips\"        \n[4] \"pci\"               \"household_income\"  \"family_income\"    \n[7] \"population\"        \"num_of_households\"\n\n# View a transposed table of your data\nglimpse(counties_income)\n\nRows: 3,231\nColumns: 8\n$ county            &lt;chr&gt; \"New York County\", \"Arlington\", \"Falls Church City\",…\n$ states            &lt;chr&gt; \"New York\", \"Virginia\", \"Virginia\", \"California\", \"C…\n$ statefips         &lt;chr&gt; \"36\", \"51\", \"51\", \"6\", \"6\", \"51\", \"8\", \"35\", \"51\", \"…\n$ pci               &lt;int&gt; 76592, 62018, 59088, 56791, 56248, 54608, 51814, 510…\n$ household_income  &lt;int&gt; 69659, 103208, 120000, 90839, 124055, 85706, 72745, …\n$ family_income     &lt;int&gt; 86553, 139244, 152857, 117357, 124055, 107511, 93981…\n$ population        &lt;int&gt; 1628706, 214861, 12731, 254643, 1927852, 143684, 171…\n$ num_of_households &lt;int&gt; 759460, 94454, 5020, 102912, 640215, 65369, 7507, 75…\n\n# Generate summaries for each of your variables\nsummary(counties_income)\n\n    county             states           statefips              pci       \n Length:3231        Length:3231        Length:3231        Min.   : 5441  \n Class :character   Class :character   Class :character   1st Qu.:19674  \n Mode  :character   Mode  :character   Mode  :character   Median :22782  \n                                                          Mean   :23268  \n                                                          3rd Qu.:26136  \n                                                          Max.   :76592  \n household_income family_income      population      num_of_households\n Min.   : 11680   Min.   : 13582   Min.   :     17   Min.   :      6  \n 1st Qu.: 37622   1st Qu.: 46942   1st Qu.:  11232   1st Qu.:   4302  \n Median : 43853   Median : 54461   Median :  25975   Median :   9792  \n Mean   : 45220   Mean   : 55752   Mean   :  97552   Mean   :  36130  \n 3rd Qu.: 50854   3rd Qu.: 62850   3rd Qu.:  65806   3rd Qu.:  25014  \n Max.   :124055   Max.   :152857   Max.   :9893481   Max.   :3230383  \n\n# Generate a summary of one variable in your datset using the $ operator\nsummary(counties_income$household_income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11680   37622   43853   45220   50854  124055",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#simple-summary-statistics",
    "href": "intro_to_r.html#simple-summary-statistics",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Simple summary statistics",
    "text": "Simple summary statistics\nNow that we’ve loaded our dataset, let’s run a simple analysis.\nThe following code block uses the summarize() function to “collapse” variables from the counties_income dataset into a smaller table with summary statistics. Specifically, we calculate the median household income and a count of counties for each state.\nIn this example\n\nmedian(household_income) calculates the median income.\nn() is a function that counts the number of observations in our sample.\n\n\nsummarize(counties_income,\n            med_hhinc = median(household_income),\n            n=n())\n\n  med_hhinc    n\n1     43853 3231\n\n\nWe can also create summary statistics by specific groupings. For example, if we want to calculate the median household income for each state, we can use the .by= argument inside summarize()\n\nsummarize(counties_income, \n          med_hhinc = median(household_income), \n          n = n(), \n          .by = states)\n\n                     states med_hhinc   n\n1                  New York   50467.5  62\n2                  Virginia   47549.0 134\n3                California   53532.0  58\n4                  Colorado   46793.5  64\n5                New Mexico   37933.0  33\n6                New Jersey   70912.0  21\n7                     Texas   44150.5 254\n8                  Maryland   66587.5  24\n9               Connecticut   68960.5   8\n10            Massachusetts   63225.0  14\n11                   Alaska   62519.0  29\n12                   Hawaii   62052.0   5\n13                  Wyoming   55347.0  23\n14                     Utah   49506.0  29\n15             Pennsylvania   46044.0  67\n16                Wisconsin   48709.5  72\n17                Tennessee   37618.0  95\n18             Rhode Island   71238.0   5\n19                     Ohio   44654.5  88\n20               Washington   47195.0  39\n21                  Indiana   46746.0  92\n22                   Kansas   45830.0 105\n23             North Dakota   50274.0  53\n24                 Illinois   47385.0 102\n25            New Hampshire   56904.5  10\n26                Minnesota   50030.0  87\n27                  Florida   43413.0  67\n28                  Georgia   37487.0 159\n29                 Michigan   41759.0  83\n30             South Dakota   47043.5  66\n31                     Iowa   48601.0  99\n32                    Idaho   42072.0  44\n33                 Missouri   39519.0 115\n34           North Carolina   41030.5 100\n35                   Nevada   52101.0  17\n36                 Kentucky   38776.0 120\n37                  Alabama   36447.0  67\n38                  Vermont   52470.0  14\n39              Mississippi   33562.0  82\n40                  Montana   43368.0  56\n41                    Maine   44327.5  16\n42                   Oregon   43524.5  36\n43           South Carolina   39271.0  46\n44                 Delaware   55149.0   3\n45                 Nebraska   45610.0  93\n46                Louisiana   40792.0  64\n47            West Virginia   37895.0  55\n48                 Oklahoma   42751.0  77\n49                 Arkansas   35153.0  75\n50                  Arizona   42987.0  15\n51      U.S. Virgin Islands   38232.0   3\n52              Puerto Rico   17434.0  78\n53                     Guam   48274.0   1\n54 Northern Mariana Islands   23125.0   3\n55           American Samoa   24027.5   4\n\n\nThis tells R to calculate the summary statistics separately for each unique value in the states column",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#creating-new-variables-with-mutate",
    "href": "intro_to_r.html#creating-new-variables-with-mutate",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Creating new variables with mutate",
    "text": "Creating new variables with mutate\nThe mutate() function adds new variables to a dataset without collapsing it, unlike summarize() which reduces the dataset to summary rows.\nIn the example below, we use mutate() to create a new column called rank, which ranks counties from highest to lowest based on their per capita income (pci).\n\nrank(-pci) ranks the values of pci in descending order (the minus sign indicates descending).\nThe result is a new dataset ranked_income where each county keeps its original data, now with an added rank column.\n\n\nranked_income &lt;- mutate(counties_income, rank = rank(-pci))\n\n# Print the top 5 rows of our new dataframe/table\nhead(ranked_income)\n\n             county     states statefips   pci household_income family_income\n1   New York County   New York        36 76592            69659         86553\n2         Arlington   Virginia        51 62018           103208        139244\n3 Falls Church City   Virginia        51 59088           120000        152857\n4             Marin California         6 56791            90839        117357\n5       Santa Clara California         6 56248           124055        124055\n6   Alexandria City   Virginia        51 54608            85706        107511\n  population num_of_households rank\n1    1628706            759460    1\n2     214861             94454    2\n3      12731              5020    3\n4     254643            102912    4\n5    1927852            640215    5\n6     143684             65369    6",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#how-to-look-up-a-functions-arguments",
    "href": "intro_to_r.html#how-to-look-up-a-functions-arguments",
    "title": "Installing R & RStudio EARN Talk",
    "section": "How to look up a function’s arguments",
    "text": "How to look up a function’s arguments\nNot sure what a function does or what arguments it accepts? You can use the help command in R to look it up!\nType a question mark (?) followed by the function name in your console or code chunk:\n\n?mutate\n\n?summarize\n\n# alternatively, you can type\nhelp(summarize)\n\nThis opens the help file for the function, which includes a short description of what the function does, a list of arguments you can use, and examples showing how to apply it. This is a great habit to adopt when trying new functions!",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#using-pipes-to-create-multi-line-commands",
    "href": "intro_to_r.html#using-pipes-to-create-multi-line-commands",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Using pipes to create multi-line commands",
    "text": "Using pipes to create multi-line commands\nIn R, the pipe operator |&gt; allows you to chain together multiple steps of a data transformation in a readable, top-to-bottom format. Each step passes its result to the next command. This makes your code easier to read and avoids creating lots of intermediate objects.\nHere’s an example using the |&gt; pipe to filter and summarize data across multiple lines:\n\n# Executing a multi-line command using pipes \ncleaned_data &lt;- counties_income |&gt;\n    # Here we use the filter() command to restrict our sample to a few states\n    filter(states %in% c('New York', 'California', 'North Carolina' )) |&gt;\n    # Use summarise (Note, summarize() and summarise() are interchangeable. British spelling fans, rejoice!)\n    summarise(mean_hhinc = mean(household_income), \n              n = n(), \n              .by = states)\n\nWhat this code does:\n\nStarts with the counties_income dataset\nFilters it to only include counties in New York, California, and North Carolina\nSummarizes the data by state, calculating:\n\nThe mean household income\nThe number of counties",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#exporting-your-results",
    "href": "intro_to_r.html#exporting-your-results",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Exporting your results",
    "text": "Exporting your results\nOnce you’ve created a cleaned or summarized dataset, you may want to save it to a file. The write.csv() function lets you export your data to a .csv file that you can open in Excel or share with others.\n\n# Writing a csv to a path\nwrite.csv(cleaned_data, \"output/state_incomes.csv\")\n\nThis line:\n\nWrites the cleaned_data dataset to a file called state_incomes.csv\nSaves it in the output/ folder within your project directory\n\nNote: Make sure the \"output\" folder already exists in your project. R will return an error if the folder doesn’t exist.",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "intro_to_r.html#additional-resources",
    "href": "intro_to_r.html#additional-resources",
    "title": "Installing R & RStudio EARN Talk",
    "section": "Additional resources",
    "text": "Additional resources\nAs you embark on your R journey, check out the Other Resources page for additional materials!\nHappy coding!",
    "crumbs": [
      "Learn R",
      "Tutorials",
      "Installing R & RStudio EARN Talk"
    ]
  },
  {
    "objectID": "rtw_union.html",
    "href": "rtw_union.html",
    "title": "Comparing union density rates in right-to-work versus non-right-to-work states",
    "section": "",
    "text": "Use this code to find union density rates for right-to-work versus non-right-to-work states using the EPI CPS microdata extracts.\n[2/10/26 UPDATE: See here for a more accurate way to assign RTW vs non-RTW labels that takes into account changes in status over time. The code below has not been updated to account for this new methodology.]\nNote: This code requires the use of an additional csv file to assign the RTW vs non-RTW labels to each state. Download the file here and save to your project before running code.\nYou can edit this file (and the relevant code below) to change how states are grouped (e.g., by region).\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(epiextractr)\nlibrary(epidatatools)\nlibrary(openxlsx)",
    "crumbs": [
      "R code",
      "Code Repository",
      "Comparing union density rates in right-to-work versus non-right-to-work states"
    ]
  },
  {
    "objectID": "rtw_union.html#set-up-your-workbook",
    "href": "rtw_union.html#set-up-your-workbook",
    "title": "Comparing union density rates in right-to-work versus non-right-to-work states",
    "section": "Set up your workbook",
    "text": "Set up your workbook\nNote: Don’t forget to update years, file names, and file paths to match your setup before running the script.\nIn this chunk, you’ll define the sheet function for the generated workbook and apply some formatting. This helps with readability and condenses the code.\n\n# Define sheet function\nsheet_fun &lt;- function(data, wb, s, format = NULL) {\n  # Add a worksheet to workbook object\n  # Note: The sheet name and wb object are anonymized\n  addWorksheet(wb, sheetName = paste0(s))\n  \n  # Write data to sheet\n  # Note: anonymize data for use in a piped method call\n  writeData(wb, sheet = paste0(s), x = data)\n  \n  # Format based on the value of format\n  if (!is.null(format)) {\n    lapply(format, function(f) {\n      switch(f,\n             \"PERCENTAGE\" = addStyle(wb, sheet = paste0(s), style = createStyle(numFmt = '0.0%'), cols = 2:ncol(data), rows = 2:(nrow(data) + 1), gridExpand = TRUE),\n             \"NUMBER\" = addStyle(wb, sheet = paste0(s), style = createStyle(numFmt = '#,##0'), cols = 2:length(data), rows = 2:(nrow(data) + 1), gridExpand = TRUE)\n      )\n    })\n  }\n  \n  # This line allows the output to return to the data.frame\n  # Note: removing this specification changes output to a list of attributes that are used to write to the wb object  \n  return(data)\n  \n}",
    "crumbs": [
      "R code",
      "Code Repository",
      "Comparing union density rates in right-to-work versus non-right-to-work states"
    ]
  },
  {
    "objectID": "rtw_union.html#import-and-clean-data",
    "href": "rtw_union.html#import-and-clean-data",
    "title": "Comparing union density rates in right-to-work versus non-right-to-work states",
    "section": "Import and clean data",
    "text": "Import and clean data\nNote: Don’t forget to update years, file names, and file paths to match your setup before running the script.\nRunning this script chunk will call the BLS Current Population Survey ORG data required to calculate union density. It will also use the geo_rtw_labels.csv file you downloaded to label each state as “RTW” or “Non-RTW.”\n\n# Set objects to download\nvar_list &lt;- c(\"year\", \"month\", \"age\", \"statefips\", \n              \"wage\", \"union\", \"lfstat\")\n\n# Load supplemental RTW label data\ngeo_rtw_labels &lt;- read.csv(\"./geo_rtw_labels.csv\")) %&gt;% \n  select(statefips, rtw, rtw_timeline_lab)\n\n# Import CPS ORG data\n# Note: make sure the years reflect your desired window!\ncps_org &lt;- load_org(1983:2024, all_of(c(var_list, \"orgwgt\"))) %&gt;%\n  # Filter out workers under 16 and self-employed workers. \n  filter(age &gt;= 16, lfstat == 1) %&gt;% \n  # Merge RTW status labels to states\n  left_join(geo_rtw_labels, by = \"statefips\")",
    "crumbs": [
      "R code",
      "Code Repository",
      "Comparing union density rates in right-to-work versus non-right-to-work states"
    ]
  },
  {
    "objectID": "rtw_union.html#run-analysis-and-download-your-output",
    "href": "rtw_union.html#run-analysis-and-download-your-output",
    "title": "Comparing union density rates in right-to-work versus non-right-to-work states",
    "section": "Run analysis and download your output",
    "text": "Run analysis and download your output\nHere you’ll run the analysis and produce the resulting Excel workbook output.\n\n# set wb object to bind to \nunion_wb &lt;- createWorkbook()\n\nunion &lt;- cps_org %&gt;% \n  # union membership rates by RTW\n  summarise(union = weighted.mean(union, w = orgwgt/12, na.rm = TRUE), .by = c(year, rtw_timeline_lab)) %&gt;% \n  # reshape wide by RTW label\n  pivot_wider(id_cols = year, names_from = rtw_timeline_lab, values_from = union) %&gt;% \n  # write to wb object\n  sheet_fun(union_wb, s = \"union_rtw\", format = \"PERCENTAGE\")\n\n# save workbooks; make sure to edit the file path!\nsaveWorkbook(union_wb, here(\"output/rtw_union.xlsx\"), overwrite = TRUE)",
    "crumbs": [
      "R code",
      "Code Repository",
      "Comparing union density rates in right-to-work versus non-right-to-work states"
    ]
  },
  {
    "objectID": "rtw_union.html#bonus-generate-median-wages",
    "href": "rtw_union.html#bonus-generate-median-wages",
    "title": "Comparing union density rates in right-to-work versus non-right-to-work states",
    "section": "Bonus: generate median wages",
    "text": "Bonus: generate median wages\nUse this chunk to generate median wages across all workers and compare to median wages in RTW vs non-RTW states.\nNote: This wage does not factor in any controls (e.g., demographics, occupation, education), so is best used for a relative comparison.\n\n# median wage for 2023 and 2024\nbinipolate(cps_org %&gt;% filter(year &gt;= 2023), wage, bin_size = 0.25, .by = year, w = orgwgt/12)\n\n# median wage by RTW status for 2024\nbinipolate(cps_org %&gt;% filter(year == 2024), wage, bin_size = 0.25, .by = c(year, rtw_timeline_lab), w = orgwgt/12)\n\nHappy coding!",
    "crumbs": [
      "R code",
      "Code Repository",
      "Comparing union density rates in right-to-work versus non-right-to-work states"
    ]
  },
  {
    "objectID": "union_premium.html",
    "href": "union_premium.html",
    "title": "Calculate union wage premiums by state and year",
    "section": "",
    "text": "Use this code to calculate a union wage premium (the difference between median union and nonunion median wages) by state using the EPI CPS microdata extracts.\nSome utility notes:\nPlease reach out to ecohn@epi.org with any questions. Now let’s get coding!\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(epiextractr)\nlibrary(epidatatools)\nlibrary(labelled)\nlibrary(realtalk)",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculate union wage premiums by state and year"
    ]
  },
  {
    "objectID": "union_premium.html#import-and-clean-data",
    "href": "union_premium.html#import-and-clean-data",
    "title": "Calculate union wage premiums by state and year",
    "section": "Import and clean data",
    "text": "Import and clean data\nNote: Don’t forget to update years to match your setup before running the script.\nRunning this script chunk will call the BLS Current Population Survey ORG data required to calculate union wage premiums.\n\n# Import CPS ORG data\n# Note: load as many years necessary to get sufficient sample sizes or desired time series.\ncps_org &lt;- load_org(2020:2024, \"year\", \"age\", \"statefips\", \"wage\", \"union\", \"orgwgt\", \"a_earnhour\", \"cow1\") %&gt;%\n  # Age and labor force restrictions (exclude self-employed and self-incorporated), non-imputed wages.\n  filter(age &gt;= 16, cow1 &lt;= 5, a_earnhour != 1, !is.na(wage))",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculate union wage premiums by state and year"
    ]
  },
  {
    "objectID": "union_premium.html#method-1-point-in-time-comparisons",
    "href": "union_premium.html#method-1-point-in-time-comparisons",
    "title": "Calculate union wage premiums by state and year",
    "section": "Method 1: Point-in-time comparisons",
    "text": "Method 1: Point-in-time comparisons\nThis method produces union wage premiums for all fifty states, pooling five years of data to get sufficient sample sizes.\nNote: some of the sample sizes are still quite small, even with five years of data. E.g., South Carolina’s union-represented sample. Consider expanding the number of years pooled, but keep in mind that this will also alter what you can say about the results.\n\nCreate wage data\nThis code chunk uses EPI methodology to correct for wage clumping by created a weighted average of wages around the median. The result is one median wage per state.\n\n# Note: divide orgwgt by as many months are in your pool.\nwage_single &lt;- cps_org |&gt;\n  mutate(union = to_factor(union)) |&gt;\n  summarise(\n      wage_median = averaged_median(\n        x = wage, \n        w = orgwgt/60,  \n        quantiles_n = 9L, \n        quantiles_w = c(1:4, 5, 4:1)),\n        n=n(),\n        .by=c(union, statefips))\n\n\n\nCalculate union wage premium\nThis code chunk separates out union versus nonunion wages, and then finds the percent difference. It also reorients the data to make them easier to read.\n\nwage_dif_single &lt;- wage_single |&gt;\n  mutate(union_stat = case_when(union == \"Not union represented\" ~ \"nonunion\", \n                                union == \"Union represented\" ~ \"union\")) |&gt;\n  pivot_wider(id_cols = statefips, names_from = union_stat, values_from = wage_median) |&gt;\n  mutate(diff = ((union-nonunion)/nonunion))|&gt;\nmutate(state = to_factor(statefips)) |&gt;\nselect(statefips, state, everything())",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculate union wage premiums by state and year"
    ]
  },
  {
    "objectID": "union_premium.html#method-2-time-series",
    "href": "union_premium.html#method-2-time-series",
    "title": "Calculate union wage premiums by state and year",
    "section": "Method 2: Time series",
    "text": "Method 2: Time series\nThis method produces inflation-adjusted union wage premiums for one state over time.\nNote: Because you can’t combine years to pool data for this method, check sample sizes before proceeding. Do not use for states that have insufficient sample sizes.\nYou can check sample sizes by running the code through the line noted below and checking the n column of wage_series.\n\nSet up inflation adjustment\nFor more information on inflation adjusting wages, see Inflation adjusting with Realtalk.\n\n# Calculate real wage over time: load CPI data from realtalk\ncpi_data &lt;- realtalk::c_cpi_u_annual\n\n# Set base year to 2024\ncpi2024 &lt;- cpi_data$c_cpi_u[cpi_data$year==2024]\n\n\n\nCreate wage data\nThis code chunk calculates median wages and adjusts them for inflation.\n\n# Note: change statefips to whichever state you prefer.\nwage_series &lt;- cps_org |&gt;\n  filter(statefips == 36) |&gt;\n  mutate(union = to_factor(union)) |&gt;\n  summarise(\n     wage_median = averaged_median(\n     x = wage, \n     w = orgwgt/12,  \n     quantiles_n = 9L, \n     quantiles_w = c(1:4, 5, 4:1)),\n     n=n(),\n     .by = c(year, union)) |&gt; #if you want to check the sample size, run your code through this line (exclude the |&gt; character)\n  # Merge annual CPI data to data frame by year\n  left_join(cpi_data, by='year') |&gt;\n  # Inflation adjust wages\n mutate(real_wage = wage_median * (cpi2024/c_cpi_u)) |&gt;\nselect(year, union, real_wage)\n\n\n\nCalculate union wage premium\nThis code chunk separates out union versus nonunion wages, and then finds the percent difference. It also reorients the data to make them easier to read.\n\nwage_dif_series &lt;- wage_series |&gt;\n  mutate(union_stat = case_when(union == \"Not union represented\" ~ \"nonunion\", \n                                union == \"Union represented\" ~ \"union\")) |&gt;\n  pivot_wider(id_cols = year, names_from = union_stat, values_from = real_wage) |&gt;\n  mutate(diff = ((union-nonunion)/nonunion)) |&gt;\n  select(year, nonunion, union, diff)\n\nHappy coding!",
    "crumbs": [
      "R code",
      "Code Repository",
      "Calculate union wage premiums by state and year"
    ]
  },
  {
    "objectID": "bin_wage_deciles.html",
    "href": "bin_wage_deciles.html",
    "title": "Wage deciles by state",
    "section": "",
    "text": "set more off\nclear all\n\n*NOTE: Users will need to create their own directory and relative directories \nglobal base \"/your_directory\"\nglobal code ${base}code/\nglobal output ${base}output/\n\n\n*load_epiextracts is an easy way to load a selection of years and variables \n* of the EPI CPS extracts into memory. First, install the Stata package with\n*See https://microdata.epi.org/basicuse/ for use information.\n\n*net install load_epiextracts, from(\"https://microdata.epi.org/stata\")\n\n* load CPS ORG: wage, wbho\nload_epiextracts, begin(2022m1) end(2022m12) sample(ORG) keep(year month orgwgt age emp selfemp wage statefips)\n\n\ntempfile allthedata\nsave `allthedata'\n\n* define sample\n\nkeep if age&gt;=16\nkeep if emp==1\nkeep if selfemp!=1 & selfemp!=.\n\n\n* Calculate wage deciles, by year and statefips\nuse `allthedata', clear\nbinipolate wage [pw=orgwgt/12], binsize(.50) by(year statefips) collapsefun(gcollapse) p(10 20 30 40 50 60 70 80 90)\n\n*Turn statefips labels into strings\ndecode statefips, gen(states)\ndrop statefips\n\n*Reshape data wide\nreshape wide wage_binned, i(year percentile) j(states) string\n\n*Export state wage deciles to csv file\nexport delim ${output}state_wage_deciles.csv, replace\n\n\n\n Back to top",
    "crumbs": [
      "Stata code",
      "Wage deciles by state"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the EARN code library, a resource dedicated to the pursuit of economic and racial justice.\nThis library is intended to be a hub for generating and sharing research ideas, enhancing data analysis accessibility and transparency, and fostering collaboration.\nOn this site, you will find coding examples, commonly used EPI/EARN methodologies for economic analysis, coding resources, and beyond. Most importantly, user contributions are warmly welcomed!",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#about-this-site",
    "href": "index.html#about-this-site",
    "title": "About",
    "section": "",
    "text": "Welcome to the EARN code library, a resource dedicated to the pursuit of economic and racial justice.\nThis library is intended to be a hub for generating and sharing research ideas, enhancing data analysis accessibility and transparency, and fostering collaboration.\nOn this site, you will find coding examples, commonly used EPI/EARN methodologies for economic analysis, coding resources, and beyond. Most importantly, user contributions are warmly welcomed!",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#user-code-submissions",
    "href": "index.html#user-code-submissions",
    "title": "About",
    "section": "User code submissions",
    "text": "User code submissions\nHave a coding sample you would like to submit to the EARN code library? Contact ecohn (at) epi (dot) org\nComponents of a code library submission are:\n\nDescription​\nCode file(s)​\nSupplementary data​\nOutput example\nAuthor contact information",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#happy-coding",
    "href": "index.html#happy-coding",
    "title": "About",
    "section": "Happy coding!",
    "text": "Happy coding!\n\n\n# A heart-shaped scatterplot\n\n# Load highcharter library\nlibrary(highcharter)\n\n# Generate heart-shaped data\nt &lt;- seq(0, 2 * pi, by = 0.01)\nx &lt;- 16 * sin(t)^3\ny &lt;- 13 * cos(t) - 5 * cos(2 * t) - 2 * cos(3 * t) - cos(4 * t)\n\n# Create our data frame\ndf &lt;- data.frame(x, y)\n\n# Create highcharter plot\nhighchart(type = \"chart\") %&gt;% \n  hc_add_series(data = df, type = \"scatter\", color = \"red\", marker = list(radius = 2)) %&gt;% \n  hc_xAxis(min = -20, max = 20) %&gt;% \n  hc_yAxis(min = -20, max = 15) %&gt;% \n  hc_chart(backgroundColor = \"transparent\") %&gt;% \n  hc_legend(enabled = FALSE)",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "rtw_definitions.html",
    "href": "rtw_definitions.html",
    "title": "Defining states by right-to-work status by year",
    "section": "",
    "text": "Use this code to define right-to-work versus non-right-to-work status for states by year, factoring in when states became or stopped being RTW. This code is best for running time series comparisons between RTW and non-RTW states. An example at the bottom of this page calculates RTW vs non-RTW prime age EPOPS over time using the EPI CPS microdata extracts.\nNote: This code requires the use of an additional csv file to assign the RTW vs non-RTW labels to each state. Download the file here and save to your project before running code.\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(epiextractr)\nlibrary(labelled)",
    "crumbs": [
      "R code",
      "Code Repository",
      "Defining states by right-to-work status by year"
    ]
  },
  {
    "objectID": "rtw_definitions.html#import-and-clean-data",
    "href": "rtw_definitions.html#import-and-clean-data",
    "title": "Defining states by right-to-work status by year",
    "section": "Import and clean data",
    "text": "Import and clean data\nNote: Don’t forget to update years, file names, and file paths to match your setup before running the script.\nIn this chunk, you’ll reformat the RTW label rtw_status_years.csv file into a format that’s easier to process in R.\n\n# read in RTW status by year\nrtw_status_year &lt;- read.csv(\"./input/rtw_status_year.csv\") |&gt; \n  pivot_longer(cols = -year, names_to = \"state\", values_to = \"rtw_status\") |&gt; \n  arrange(state, year)\n\nRunning this script chunk will call and clean the BLS Current Population Survey Basic data required to calculate prime age EPOPs. It will also join the rtw_status_years data frame you created to label each state as “RTW” or “Non-RTW” in a given year.\n\n# import CPS Basic data\ncps_basic &lt;- load_basic(2000:2025, \"year\", \"month\", \"selfemp\", \"selfinc\", \"age\", \"statefips\", \"emp\", \"lfstat\",\n              \"cow1\", \"female\", \"wbhao\", \"basicwgt\") |&gt; \n  # Age restrictions\n  filter(age &gt;= 16) |&gt; \n  # identify prime-age (for EPOPs)\n  mutate(prime_age = case_when(between(age, 25, 54) ~ 1, TRUE ~ 0), state=to_factor(statefips)) |&gt; \n  # merge RTW status\n  left_join(rtw_status_year, by = c(\"year\", \"state\")) |&gt; \n  # adjust for missing Oct data\n  mutate(weight = case_match(year, \n                             2025 ~ basicwgt / 11, \n                             .default = basicwgt / 12))",
    "crumbs": [
      "R code",
      "Code Repository",
      "Defining states by right-to-work status by year"
    ]
  },
  {
    "objectID": "rtw_definitions.html#run-analysis",
    "href": "rtw_definitions.html#run-analysis",
    "title": "Defining states by right-to-work status by year",
    "section": "Run analysis",
    "text": "Run analysis\nHere you’ll run the analysis to calculate prime age EPOPs by RTW vs. non-RTW states over time.\n\nepop_rtw &lt;- cps_basic |&gt; \n  filter(prime_age == 1) |&gt; \n  # EPOP = weighted mean of employment\n  summarise(epop = weighted.mean(emp, w = weight, na.rm = TRUE), .by = c(year, rtw_status)) |&gt; \n  # reshape wide by RTW label\n  pivot_wider(id_cols = year, names_from = rtw_status, values_from = epop) |&gt; \n  rename(RTW = `1`, non_RTW = `0`)\n\nAnd that’s all! You can easily apply the first two code chunks to any CPS (or other survey) analysis you want to do.\nHappy coding!",
    "crumbs": [
      "R code",
      "Code Repository",
      "Defining states by right-to-work status by year"
    ]
  },
  {
    "objectID": "epi_microdata.html",
    "href": "epi_microdata.html",
    "title": "Load EPI CPS Extracts via epiextractr",
    "section": "",
    "text": "Note: Users will need to install epiextractr for this example. Refer to EPI packages for R for installation instructions.\n\nLoad required libraries\n\nlibrary(tidyverse)\nlibrary(epiextractr)\n\n\n\nDownload CPS files\n\n# download CPS ORG files  \ndownload_cps( sample = 'org', extracts_dir = 'C:/YOUR_PATH/cps', overwrite = TRUE)\n\n# download CPS Basic (4085.3 MB) \ndownload_cps(sample='basic', extracts_dir ='C:/YOUR_PATH/cps', overwrite = TRUE)\n\n# download CPS May (38.8 MB)\ndownload_cps(sample='may', extracts_dir = 'C:/YOUR_PATH/cps', overwrite = TRUE)\n\nThis will download the latest EPI CPS ORG extracts in .feather format from https://microdata.epi.org and place them in the directory C:\\data\\cps.\n\n\nLoad your CPS extracts!\nAfter the data is downloaded, load a selection of CPS data for your analysis:\n\norg &lt;- load_cps(\"org\", 2010:2022, year, orgwgt, wage, age, statefips, wbho, \n                .extracts_dir = 'C:/YOUR_PATH/cps')\n\n\n\n(Optional) Set and forget CPS data files by creating/editing an .Renviron file\nTo simplify usage, you can omit the .extracts_dir argument by setting the environment variables to your extracts directory. This allows you to call CPS extracts from a single, dedicated folder, eliminating redundant downloads of CPS files.\n\n# Find the .Renviron file\nrenviron_path &lt;- file.path(Sys.getenv(\"HOME\"), \".Renviron\")\n\n# Open the file for editing\nfile.edit(renviron_path)\n\n# Paste the environment variable settings into your .Renviron file,\n# and make sure the paths are set to the location of your CPS files\n\nEPIEXTRACTS_CPSBASIC_DIR=C:/YOUR_PATH/cps\nEPIEXTRACTS_CPSMAY_DIR=C:/YOUR_PATH/cps\nEPIEXTRACTS_CPSORG_DIR=C:/YOUR_PATH/cps\n\nAfter editing your .Renviron file, save your changes and restart R to apply them.\nIf you’ve set your .Renviron file paths to point to the folder containing your CPS files, you can omit the .extracts_dir() command\n\norg &lt;- load_cps(\"org\", 2010:2022, year, orgwgt, wage, age, statefips, wbho)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R code",
      "Load EPI CPS Extracts via epiextractr"
    ]
  },
  {
    "objectID": "union_density.html",
    "href": "union_density.html",
    "title": "Historical state union density",
    "section": "",
    "text": "This script uses Current Population Survey (CPS) microdata extracts from https://microdata.epi.org/ to calculate union density from 1983-Present.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(epiextractr)\n\nLoad CPS data using epiextractr\n\nbasic &lt;- load_basic(1983:2022, year, month, statefips, basicwgt, union, unmem, age, emp, selfemp, selfinc) %&gt;%\n  filter(age&gt;=16, emp==1) %&gt;%\n  #remove self-employed and self-incorporated workers from sample\n  mutate(selfemp0 = ifelse(selfemp==1 & !is.na(selfemp), yes=1, no=0),\n         selfinc0 = ifelse(selfinc==1 & !is.na(selfinc), yes=1, no=0)) %&gt;%\n  filter(selfemp0==0, selfinc0==0)\n\nUsing EPI CPS Basic Monthly Extracts, Version 2025.9.11\n\n\nCalculate US union density 1983–2022\n\n#US union members and union represented by year, 1983-2022\n\ndensity_us &lt;- basic %&gt;% \n  group_by(year) %&gt;% \n  summarise(represented_share = weighted.mean(union, w=basicwgt/12, na.rm=TRUE),\n            rep_n = sum(union, na.rm=TRUE),\n            member_share = weighted.mean(unmem, w=basicwgt/12, na.rm=TRUE),\n            memb_n = sum(unmem, na.rm=TRUE),\n            wgt_memb = sum(unmem * basicwgt/12, na.rm=TRUE))\n\ndensity_us\n\n# A tibble: 40 × 6\n    year represented_share rep_n member_share memb_n wgt_memb\n   &lt;int&gt;             &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1  1983           NaN         0        0.201  34258 4422628.\n 2  1984             0.216 38008        0.188  32921 4328307.\n 3  1985             0.205 36788        0.180  32185 4236957.\n 4  1986             0.199 35321        0.175  31013 4230555.\n 5  1987             0.192 34505        0.170  30488 4221013.\n 6  1988             0.190 32242        0.168  28406 4241194.\n 7  1989             0.186 32079        0.164  28261 4235755.\n 8  1990             0.182 34110        0.160  29898 4192916.\n 9  1991             0.181 32920        0.160  29051 4152415.\n10  1992             0.177 31725        0.157  28022 4098902.\n# ℹ 30 more rows\n\n\nCalculate state level union representation, 1983–2022\n\n#Union representation by year and state, 1983–Present\ndensity_state &lt;- basic %&gt;% \nsummarise(represented_share = weighted.mean(union, w=basicwgt/12, na.rm=TRUE),\n          .by = c(year, statefips)) %&gt;%\n  \n  #Turn statefips labels into strings\n  mutate(statefips = haven::as_factor(statefips)) %&gt;% \n  #sort by year and state\n  arrange(year, statefips) %&gt;% \n  #reshape data\n  pivot_wider(id_cols = year, names_from = statefips, values_from = represented_share)\n\ndensity_state\n\n# A tibble: 40 × 52\n    year      AL      AK       AZ      AR      CA      CO      CT      DE\n   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1983 NaN     NaN     NaN      NaN     NaN     NaN     NaN     NaN    \n 2  1984   0.179   0.279   0.120    0.120   0.248   0.146   0.222   0.193\n 3  1985   0.177   0.280   0.121    0.133   0.239   0.137   0.222   0.185\n 4  1986   0.181   0.267   0.109    0.123   0.231   0.148   0.207   0.190\n 5  1987   0.155   0.256   0.0825   0.133   0.223   0.138   0.200   0.180\n 6  1988   0.161   0.268   0.0863   0.117   0.216   0.124   0.205   0.167\n 7  1989   0.157   0.249   0.0877   0.121   0.218   0.113   0.193   0.173\n 8  1990   0.145   0.258   0.0941   0.121   0.209   0.120   0.196   0.168\n 9  1991   0.155   0.239   0.0979   0.125   0.206   0.118   0.200   0.185\n10  1992   0.162   0.221   0.0952   0.110   0.209   0.120   0.189   0.184\n# ℹ 30 more rows\n# ℹ 43 more variables: DC &lt;dbl&gt;, FL &lt;dbl&gt;, GA &lt;dbl&gt;, HI &lt;dbl&gt;, ID &lt;dbl&gt;,\n#   IL &lt;dbl&gt;, IN &lt;dbl&gt;, IA &lt;dbl&gt;, KS &lt;dbl&gt;, KY &lt;dbl&gt;, LA &lt;dbl&gt;, ME &lt;dbl&gt;,\n#   MD &lt;dbl&gt;, MA &lt;dbl&gt;, MI &lt;dbl&gt;, MN &lt;dbl&gt;, MS &lt;dbl&gt;, MO &lt;dbl&gt;, MT &lt;dbl&gt;,\n#   NE &lt;dbl&gt;, NV &lt;dbl&gt;, NH &lt;dbl&gt;, NJ &lt;dbl&gt;, NM &lt;dbl&gt;, NY &lt;dbl&gt;, NC &lt;dbl&gt;,\n#   ND &lt;dbl&gt;, OH &lt;dbl&gt;, OK &lt;dbl&gt;, OR &lt;dbl&gt;, PA &lt;dbl&gt;, RI &lt;dbl&gt;, SC &lt;dbl&gt;,\n#   SD &lt;dbl&gt;, TN &lt;dbl&gt;, TX &lt;dbl&gt;, UT &lt;dbl&gt;, VT &lt;dbl&gt;, VA &lt;dbl&gt;, WA &lt;dbl&gt;, …\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R code",
      "Code Repository",
      "Historical state union density"
    ]
  },
  {
    "objectID": "inflation_adjusting.html",
    "href": "inflation_adjusting.html",
    "title": "Inflation adjusting with Realtalk",
    "section": "",
    "text": "Note: Users will need to install realtalk and epiextractr for this example. Refer to EPI packages for R for installation instructions.",
    "crumbs": [
      "R code",
      "Inflation adjusting with Realtalk"
    ]
  },
  {
    "objectID": "inflation_adjusting.html#loading-cpi-indices-using-the-realtalk-library",
    "href": "inflation_adjusting.html#loading-cpi-indices-using-the-realtalk-library",
    "title": "Inflation adjusting with Realtalk",
    "section": "Loading CPI Indices using the realtalk library",
    "text": "Loading CPI Indices using the realtalk library\nThe following chunk of code loads the R libraries necessary for this exercise. You may need to install them to run this code.\n\n#Load necessary libraries\nlibrary(tidyverse)\nlibrary(realtalk)\nlibrary(epiextractr)\nlibrary(here)\n\nThe RealTalk package includes several datasets of common US price indices. You may view those by executing the available_price_indexes() command.\n\n#list available cpi series\nrealtalk::available_price_indexes\n\n# A tibble: 20 × 6\n   index_name        frequency seasonal min_date max_date package_data_name     \n   &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                 \n 1 C-CPI-U           annual    &lt;NA&gt;     2000     2024     c_cpi_u_annual        \n 2 C-CPI-U           monthly   NSA      Dec 1999 Jan 2025 c_cpi_u_monthly_nsa   \n 3 C-CPI-U           quarterly NSA      2000q1   2024q4   c_cpi_u_quarterly_nsa \n 4 C-CPI-U, extended annual    &lt;NA&gt;     1937     2024     c_cpi_u_extended_annu…\n 5 C-CPI-U, extended monthly   NSA      Jan 1937 Jan 2025 c_cpi_u_extended_mont…\n 6 C-CPI-U, extended monthly   SA       Jan 1947 Jan 2025 c_cpi_u_extended_mont…\n 7 C-CPI-U, extended quarterly NSA      1937q1   2024q4   c_cpi_u_extended_quar…\n 8 C-CPI-U, extended quarterly SA       1947q1   2024q4   c_cpi_u_extended_quar…\n 9 CPI-U             annual    &lt;NA&gt;     1937     2024     cpi_u_annual          \n10 CPI-U             monthly   NSA      Jan 1937 Jan 2025 cpi_u_monthly_nsa     \n11 CPI-U             monthly   SA       Jan 1947 Jan 2025 cpi_u_monthly_sa      \n12 CPI-U             quarterly NSA      1937q1   2024q4   cpi_u_quarterly_nsa   \n13 CPI-U             quarterly SA       1947q1   2024q4   cpi_u_quarterly_sa    \n14 CPI-U-RS          annual    &lt;NA&gt;     1978     2023     cpi_u_rs_annual       \n15 CPI-U-RS          monthly   NSA      Dec 1977 Dec 2023 cpi_u_rs_monthly_nsa  \n16 CPI-U-X1          annual    &lt;NA&gt;     1967     1982     cpi_u_x1_annual       \n17 CPI-U-X1          monthly   NSA      Jan 1967 Dec 1982 cpi_u_x1_monthly_nsa  \n18 PCE               annual    &lt;NA&gt;     1929     2024     pce_annual            \n19 PCE               monthly   SA       Jan 1959 Dec 2024 pce_monthly_sa        \n20 PCE               quarterly SA       1947q1   2024q4   pce_quarterly_sa      \n\n\nEPI uses the CPI-U-RS series to inflation adjust wages, so we’ll select that series and assign it to a dataframe.\n\n#this creates a dataframe with the annual CPI-U-RS index from 1937-2022\ncpi_data &lt;- realtalk::cpi_u_rs_annual\n\n#Set base year to 2022\ncpi2022 &lt;- cpi_data$cpi_u_rs[cpi_data$year==2022]",
    "crumbs": [
      "R code",
      "Inflation adjusting with Realtalk"
    ]
  },
  {
    "objectID": "inflation_adjusting.html#a-refresher-on-inflation-adjustment",
    "href": "inflation_adjusting.html#a-refresher-on-inflation-adjustment",
    "title": "Inflation adjusting with Realtalk",
    "section": "A refresher on inflation adjustment",
    "text": "A refresher on inflation adjustment\nBefore jumping into the full code. Let’s refresh on how to calculate inflation using the Consumer Price Index.\nInflation in a given year is calculated by dividing the price of a market basket in a particular year by the price of the same basket in the base year, like so:\n\\[\n\\frac{\\text{Given year}}{\\text{Base year}} * 100\n\\]\nFor example, let’s calculate how much the CPI-U-RS index has increased from 1978 to 2022.\n\\[\n\\frac{\\text{CPI}_{2022}}{\\text{CPI}_{1978}} = \\frac{431.5}{104} \\approx 4.149038 \\text{ or } 414.9\\%\n\\]\nAnd voila - we see that inflation has caused the basket of goods to increase 414.9% since 1978.",
    "crumbs": [
      "R code",
      "Inflation adjusting with Realtalk"
    ]
  },
  {
    "objectID": "inflation_adjusting.html#applying-inflation-adjustment-to-cps-org-wage-data",
    "href": "inflation_adjusting.html#applying-inflation-adjustment-to-cps-org-wage-data",
    "title": "Inflation adjusting with Realtalk",
    "section": "Applying inflation adjustment to CPS ORG wage data",
    "text": "Applying inflation adjustment to CPS ORG wage data\nThis section uses epiextractr to load Current Population Survey data. Refer to the Using EPI’s CPS Extracts to learn how to use this library.\nBelow, I load CPS ORG data and define my sample:\n\norg &lt;- load_org(2012:2022, year, month, orgwgt, wage, age, lfstat) %&gt;% \n    #define sample universe\n    filter(age&gt;=16, lfstat %in% c(1,2))\n\nNext, calculate median wages in the CPS. and merge the annual CPI index to the dataframe using left_join()\n\n#Calculate median wages in the CPS ORG\nwage_data &lt;- org %&gt;% \n  #use MetricsWeighted package to calculate a weighted median\n  #Note: I am pooling 12 months of CPS data, \n  #so I adjust orgwgt—the survey weight— variable, dividing it by 12.\n  summarize(nominal_median_wage = \n               MetricsWeighted::weighted_median(wage, w=orgwgt/12, na.rm=TRUE),\n            .by=year) %&gt;% \n\n#Merge annual CPI data to dataframe by year.\n  left_join(cpi_data, by='year') \n\nwage_data\n\n# A tibble: 11 × 3\n    year nominal_median_wage cpi_u_rs\n   &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n 1  2012                16       337.\n 2  2013                16.4     342 \n 3  2014                16.8     348.\n 4  2015                17       348.\n 5  2016                17.5     353.\n 6  2017                18       360.\n 7  2018                18.8     369.\n 8  2019                19.2     376.\n 9  2020                21       381.\n10  2021                21.4     399.\n11  2022                22.9     432.\n\n\nFinally, calculate the inflation rate relative to 2022, and the inflation adjusted median wage using the CPI index as follows:\n\nadjusted_data &lt;- wage_data %&gt;% \n \n  #This mutate command calculates inflation in a given year, relative to 2022\n  #and multiplies nominal median wages by the inflation quotient\n  mutate(infl_rel_to_2022 = signif(x = ((cpi2022/cpi_u_rs)-1), digits=2),\n         real_wage_2022 = nominal_median_wage*(cpi2022/cpi_u_rs)) %&gt;% \n  \n  relocate(infl_rel_to_2022, nominal_median_wage, .after=cpi_u_rs)\n  \nadjusted_data\n\n# A tibble: 11 × 5\n    year cpi_u_rs infl_rel_to_2022 nominal_median_wage real_wage_2022\n   &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1  2012     337.            0.28                 16             20.5\n 2  2013     342             0.26                 16.4           20.7\n 3  2014     348.            0.24                 16.8           20.8\n 4  2015     348.            0.24                 17             21.1\n 5  2016     353.            0.22                 17.5           21.4\n 6  2017     360.            0.2                  18             21.6\n 7  2018     369.            0.17                 18.8           21.9\n 8  2019     376.            0.15                 19.2           22.1\n 9  2020     381.            0.13                 21             23.8\n10  2021     399.            0.081                21.4           23.1\n11  2022     432.            0                    22.9           22.9",
    "crumbs": [
      "R code",
      "Inflation adjusting with Realtalk"
    ]
  }
]